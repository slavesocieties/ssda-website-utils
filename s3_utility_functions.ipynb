{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp s3_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "precise-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "several-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_server(outfile, server, default):\n",
    "    if server == None:\n",
    "        outfile.write(default)\n",
    "    else:\n",
    "        outfile.write(server)\n",
    "\n",
    "def manifest_from_cs(path_to_cs_batch, list_of_images, output_dir_prefix=None, manifest_server=None, image_server=None, default_server=None):\n",
    "    '''\n",
    "    Creates a IIIF manifests for volumes based on metadata from a CloudSearch upload.\n",
    "        path_to_cs_batch: Path to a JSON file containing a CloudSearch document batch **modified to include the \"volumes\" key**\n",
    "        list_of_images: a list of dictionaries in which each dictionary contains the filename and dimensions for a single image\n",
    "        output_dir_prefix (optional): path to append output manifest file name to, defaults to local\n",
    "        manifest_server (optional): base URL for manifest server, including trailing forward slash\n",
    "        \n",
    "        returns: number of manifests created\n",
    "    '''    \n",
    "    import json\n",
    "    \n",
    "    manifests = 0\n",
    "    \n",
    "    if default_server == None:\n",
    "        default_server = \"https://images.slavesocieties.org/\"   \n",
    "    \n",
    "    with open(path_to_cs_batch, encoding=\"utf-8\") as jsonfile:\n",
    "        data = json.load(jsonfile)\n",
    "        \n",
    "    for volume in data[\"volumes\"]:\n",
    "        if output_dir_prefix != None:\n",
    "            volume_json_path = output_dir_prefix + '\\\\' + volume[\"id\"] + \".json\"\n",
    "        else:\n",
    "            volume_json_path = volume[\"id\"] + \".json\"   \n",
    "    \n",
    "        with open(volume_json_path, \"w\", encoding = \"utf-8\") as outfile:\n",
    "            outfile.write('{\\n')\n",
    "            #add indentation eventually?\n",
    "            outfile.write(\"\\\"@context\\\": \\\"http://iiif.io/api/presentation/2/context.json\\\",\\n\")\n",
    "            outfile.write(\"\\\"@type\\\": \\\"sc:Manifest\\\",\\n\")\n",
    "            outfile.write(\"\\\"@id\\\": \\\"\")\n",
    "            write_server(outfile, manifest_server, default_server)        \n",
    "            outfile.write(\"manifest/\" + volume[\"id\"] + \".json\\\",\\n\")\n",
    "            if (\"title\" in volume[\"fields\"]) and (volume[\"fields\"][\"title\"] != None):\n",
    "                outfile.write(\"\\\"label\\\": \\\"\" + volume[\"fields\"][\"title\"].replace(\"\\\"\", \"\\\\\\\"\") + \"\\\",\\n\")\n",
    "            else:\n",
    "                outfile.write(\"\\\"label\\\": \\\"\\\",\\n\")\n",
    "            if \"description\" in volume[\"fields\"]:\n",
    "                outfile.write(\"\\\"description\\\": \\\"\" + volume[\"fields\"][\"description\"].replace(\"\\\"\", \"\\\\\\\"\").replace(\"  \", ' ') + \"\\\",\\n\")\n",
    "            else:\n",
    "                outfile.write(\"\\\"description\\\": \\\"\\\",\\n\")\n",
    "            outfile.write(\"\\\"metadata\\\": [\\n\")\n",
    "            outfile.write(\"{\\n\")\n",
    "            outfile.write(\"\\\"label\\\": \\\"Title\\\",\\n\")\n",
    "            if (\"title\" in volume[\"fields\"]) and (volume[\"fields\"][\"title\"] != None):\n",
    "                outfile.write(\"\\\"value\\\": \\\"\" + volume[\"fields\"][\"title\"].replace(\"\\\"\", \"\\\\\\\"\") + \"\\\"\\n\")\n",
    "            else:\n",
    "                outfile.write(\"\\\"value\\\": \\\"\\\"\\n\")\n",
    "            outfile.write(\"},\\n\")        \n",
    "            if \"creator\" in volume[\"fields\"]:\n",
    "                outfile.write(\"{\\n\")\n",
    "                outfile.write(\"\\\"label\\\": \\\"Creator\\\",\\n\")\n",
    "                outfile.write(\"\\\"value\\\": \\\"\" + volume[\"fields\"][\"creator\"].replace(\"  \", ' ') + \"\\\"\\n\")        \n",
    "                outfile.write(\"},\\n\")        \n",
    "            if (\"subject\" in volume[\"fields\"]) and (volume[\"fields\"][\"subject\"] != None):\n",
    "                vol_subject = ''\n",
    "                for subject in volume[\"fields\"][\"subject\"]:\n",
    "                    if len(vol_subject) > 0:\n",
    "                        vol_subject += '; '\n",
    "                    vol_subject += subject           \n",
    "                outfile.write(\"{\\n\")\n",
    "                outfile.write(\"\\\"label\\\": \\\"Subject\\\",\\n\")\n",
    "                outfile.write(\"\\\"value\\\": \\\"\" + vol_subject + \"\\\"\\n\")\n",
    "                outfile.write(\"},\\n\")\n",
    "            outfile.write(\"{\\n\")\n",
    "            outfile.write(\"\\\"label\\\": \\\"Digitized by\\\",\\n\")\n",
    "            if \"publisher\" in volume[\"fields\"]:\n",
    "                outfile.write(\"\\\"value\\\": \\\"\" + volume[\"fields\"][\"publisher\"] + \"\\\"\\n\")\n",
    "            else:\n",
    "                outfile.write(\"\\\"value\\\": \\\"Slave Societies Digital Archive\\\"\\n\")\n",
    "            outfile.write(\"},\\n\")\n",
    "            outfile.write(\"{\\n\")\n",
    "            outfile.write(\"\\\"label\\\": \\\"Identifier\\\",\\n\")\n",
    "            outfile.write(\"\\\"value\\\": \\\"\" + volume[\"id\"] + \"\\\"\\n\")\n",
    "            outfile.write(\"},\\n\")\n",
    "            if (\"start_date\" in volume[\"fields\"]) and (\"end_date\" in volume[\"fields\"]):\n",
    "                outfile.write(\"{\\n\")\n",
    "                outfile.write(\"\\\"label\\\": \\\"Date\\\",\\n\")\n",
    "                outfile.write(\"\\\"value\\\": \\\"\" + volume[\"fields\"][\"start_date\"][:volume[\"fields\"][\"start_date\"].find('T')] + '-' + volume[\"fields\"][\"end_date\"][:volume[\"fields\"][\"end_date\"].find('T')] + \"\\\"\\n\")\n",
    "                outfile.write(\"},\\n\")\n",
    "            vol_lang = ''\n",
    "            if \"language\" in volume[\"fields\"]:            \n",
    "                for language in volume[\"fields\"][\"language\"]:\n",
    "                    if len(vol_lang) > 0:\n",
    "                        vol_lang += '; '\n",
    "                    vol_lang += language\n",
    "            outfile.write(\"{\\n\")\n",
    "            outfile.write(\"\\\"label\\\": \\\"Language\\\",\\n\")\n",
    "            outfile.write(\"\\\"value\\\": \\\"\" + vol_lang + \"\\\"\\n\")\n",
    "            outfile.write(\"}\\n\")\n",
    "            outfile.write(\"],\\n\")            \n",
    "\n",
    "            outfile.write(\"\\\"attribution\\\": \\\"Slave Societies Digital Archive\\\",\\n\")\n",
    "            outfile.write(\"\\\"logo\\\": \\\"\")\n",
    "            write_server(outfile, image_server, default_server) \n",
    "            outfile.write(\"iiif/3/ssda_logo_horizontal.jpg/full/max/0/default.jpg\\\",\\n\")\n",
    "\n",
    "            outfile.write(\"\\\"sequences\\\": [\\n\")\n",
    "            outfile.write(\"{\\n\")\n",
    "            outfile.write(\"\\\"@type\\\": \\\"sc:Sequence\\\",\\n\")\n",
    "            outfile.write(\"\\\"@id\\\": \\\"\")\n",
    "            write_server(outfile, manifest_server, default_server)           \n",
    "            outfile.write(\"sequence/\" + volume[\"id\"] + \".json\\\",\\n\")\n",
    "            outfile.write(\"\\\"canvases\\\": [\\n\")\n",
    "\n",
    "            first_image = True\n",
    "            for image in list_of_images:\n",
    "                if not first_image:\n",
    "                    outfile.write(\",\\n\")\n",
    "                else:\n",
    "                    first_image = False\n",
    "\n",
    "                outfile.write(\"{\\n\")\n",
    "                outfile.write(\"\\\"@type\\\": \\\"sc:Canvas\\\",\\n\")            \n",
    "                outfile.write(\"\\\"@id\\\": \\\"\")\n",
    "                write_server(outfile, manifest_server, default_server)\n",
    "                image_number = int(image[\"file name\"][image[\"file name\"].find('-') + 1:image[\"file name\"].find('.')])\n",
    "                image_id = \"0\" * (4 - len(str(image_number))) + str(image_number)\n",
    "                canvas_id = volume[\"id\"] + '-' + image_id\n",
    "                outfile.write(\"canvas/\" + canvas_id + \".json\\\",\\n\")\n",
    "                outfile.write(\"\\\"label\\\": \\\"\" + str(image_number) + \"\\\",\\n\")\n",
    "                outfile.write(\"\\\"width\\\": \" + str(image[\"width\"]) + \",\\n\")\n",
    "                outfile.write(\"\\\"height\\\": \" + str(image[\"height\"]) + \",\\n\")\n",
    "                outfile.write(\"\\\"images\\\": [\\n\")\n",
    "                outfile.write(\"{\\n\")\n",
    "                outfile.write(\"\\\"@type\\\": \\\"oa:Annotation\\\",\\n\")            \n",
    "                outfile.write(\"\\\"@id\\\": \\\"\")\n",
    "                write_server(outfile, manifest_server, default_server)\n",
    "                outfile.write(\"annotation/\" + str(canvas_id) + \".json\\\",\\n\")\n",
    "                outfile.write(\"\\\"motivation\\\": \\\"sc:painting\\\",\\n\")\n",
    "                outfile.write(\"\\\"on\\\": \\\"\")\n",
    "                write_server(outfile, manifest_server, default_server)\n",
    "                outfile.write(\"canvas/\" + canvas_id + \".json\\\",\\n\")\n",
    "                outfile.write(\"\\\"resource\\\": {\\n\")            \n",
    "                outfile.write(\"\\\"@type\\\": \\\"dctypes:Image\\\",\\n\")            \n",
    "                outfile.write(\"\\\"format\\\": \\\"image/jpg\\\",\\n\")\n",
    "                outfile.write(\"\\\"@id\\\": \\\"\")\n",
    "                write_server(outfile, image_server, default_server)\n",
    "                outfile.write(\"iiif/3/\" + canvas_id + \".jpg/full/max/0/default.jpg\\\",\\n\")\n",
    "                outfile.write(\"\\\"width\\\": \" + str(image[\"width\"]) + \",\\n\")\n",
    "                outfile.write(\"\\\"height\\\": \" + str(image[\"height\"]) + \",\\n\")\n",
    "                outfile.write(\"\\\"service\\\": {\\n\")\n",
    "                outfile.write(\"\\\"@id\\\": \\\"\")\n",
    "                write_server(outfile, image_server, default_server)\n",
    "                outfile.write( \"iiif/3/\" + canvas_id + \".jpg\\\",\\n\")\n",
    "                outfile.write(\"\\\"@context\\\": \\\"http://iiif.io/api/image/3/context.json\\\",\\n\")\n",
    "                outfile.write(\"\\\"profile\\\": \\\"http://iiif.io/api/image/3/level2.json\\\"\\n\")\n",
    "                outfile.write(\"}\\n\")\n",
    "                outfile.write(\"}\\n\")\n",
    "                outfile.write(\"}\\n\")\n",
    "                outfile.write(\"]\\n\")\n",
    "                outfile.write(\"}\")\n",
    "\n",
    "            outfile.write(\"\\n\")\n",
    "            outfile.write(\"]\\n\")\n",
    "            outfile.write(\"}\\n\")\n",
    "            outfile.write(\"]\\n\")\n",
    "            outfile.write(\"}\\n\")\n",
    "            \n",
    "        manifests += 1\n",
    "    \n",
    "    return manifests\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "floating-disaster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifest_from_cs(\"la_aurora_cs.json\", images, output_dir_prefix=None, manifest_server=\"https://ssda-iiif.s3.amazonaws.com/\", image_server=\"https://images.slavesocieties.org/\", default_server=\"https://images.slavesocieties.org/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "athletic-forest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'width': 6000, 'height': 4000, 'file name': '702001-0001.jpg'}, {'width': 6000, 'height': 4000, 'file name': '702001-0002.jpg'}, {'width': 6000, 'height': 4000, 'file name': '702001-0003.jpg'}, {'width': 6000, 'height': 4000, 'file name': '702001-0004.jpg'}, {'width': 6000, 'height': 4000, 'file name': '702001-0005.jpg'}]\n"
     ]
    }
   ],
   "source": [
    "print(images[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "electric-punch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def bucket_copy(source_bucket_name, target_bucket_name, source_object_prefix = \"\"):\n",
    "    s3_resource = boto3.resource(\"s3\")\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    \n",
    "    source_bucket = s3_resource.Bucket(source_bucket_name)\n",
    "    copied_objects = 0\n",
    "    \n",
    "    for obj in source_bucket.objects.filter(Prefix = source_object_prefix):\n",
    "        copy_source = {\"Bucket\": source_bucket_name, \"Key\": obj.key}\n",
    "        if \"Process\" in obj.key:\n",
    "            s3_client.download_file(source_bucket_name, obj.key, \"temp.jpg\")\n",
    "            image_size = os.stat('temp.jpg').st_size\n",
    "            while image_size > 3000000:\n",
    "                im = Image.open(\"temp.jpg\")\n",
    "                width, height = im.size\n",
    "                im = im.resize((int(round(width * .75)), int(round(height * .75))))\n",
    "                im.save(\"temp.jpg\")\n",
    "                image_size = os.stat(\"temp.jpg\").st_size\n",
    "            s3_client.upload_file(\"temp.jpg\", target_bucket_name, obj.key[obj.key.index(\"/\") + 1:])            \n",
    "        else:\n",
    "            s3_client.copy(copy_source, target_bucket_name, obj.key)\n",
    "        copied_objects += 1\n",
    "        \n",
    "    if \"Process\" in obj.key:\n",
    "        os.remove(\"temp.jpg\")\n",
    "        \n",
    "    return str(copied_objects) + \" objects copied from \" + source_bucket_name + \" to \" + target_bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "appropriate-document",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'724 objects copied from ssda-assets to ssda-misc'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_copy(\"ssda-assets\", \"ssda-misc\", source_object_prefix = \"la-aurora/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "anticipated-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_test\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "s3_client = boto3.client(\"s3\")\n",
    "import rawpy\n",
    "import imageio\n",
    "    \n",
    "source_bucket = s3_resource.Bucket(\"ssda-misc\")\n",
    "\n",
    "for obj in source_bucket.objects.filter(Prefix = \"la-aurora\"):    \n",
    "    if obj.key.endswith(\"CR2\"):\n",
    "        s3_client.download_file(\"ssda-misc\", obj.key, \"temp.cr2\")\n",
    "        with rawpy.imread(\"temp.cr2\") as raw:                \n",
    "            rgb = raw.postprocess()        \n",
    "        imageio.imsave(\"temp.jpg\", rgb)        \n",
    "        image_size = os.stat(\"temp.jpg\").st_size\n",
    "        while image_size > 3000000:\n",
    "            im = Image.open(\"temp.jpg\")\n",
    "            width, height = im.size\n",
    "            im = im.resize((int(round(width * .75)), int(round(height * .75))))\n",
    "            im.save(\"temp.jpg\")\n",
    "            image_size = os.stat(\"temp.jpg\").st_size\n",
    "        s3_client.upload_file(\"temp.jpg\", \"ssda-misc\", obj.key[:obj.key.find(\"CR2\")] + \"jpg\")        \n",
    "        \n",
    "os.remove(\"temp.cr2\")\n",
    "os.remove(\"temp.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "arranged-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_test\n",
    "\n",
    "for obj in source_bucket.objects.filter(Prefix = \"la-aurora\"):\n",
    "    if obj.key.endswith(\"jpg\"):\n",
    "        s3_client.download_file(\"ssda-misc\", obj.key, obj.key)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alien-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def copy_jpgs(json_path, source_bucket, target_bucket):\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    images = 0\n",
    "    \n",
    "    with open(json_path, encoding=\"utf-8\") as jsonfile:\n",
    "        data = json.load(jsonfile)\n",
    "        \n",
    "    for volume in data[\"volumes\"]:\n",
    "        print(\"Now working on \" + volume[\"identifier\"])\n",
    "        for image in volume[\"images\"]:            \n",
    "            #copy_source = {\"Bucket\": source_bucket, \"Key\": volume[\"s3_path\"] + \"/JPG/\" + str(image[\"file_name\"]) + \".JPG\"}\n",
    "            try:\n",
    "                s3_client.download_file(source_bucket, volume[\"s3_path\"] + \"/JPG/\" + str(image[\"file_name\"]) + \".JPG\", \"temp.jpg\")\n",
    "            except:\n",
    "                s3_client.download_file(source_bucket, volume[\"s3_path\"] + \"/JPG/\" + str(image[\"file_name\"]) + \".jpg\", \"temp.jpg\")            \n",
    "            image_size = os.stat(\"temp.jpg\").st_size\n",
    "            while (image_size > 3000000):                \n",
    "                im = Image.open(\"temp.jpg\")\n",
    "                width, height = im.size\n",
    "                im = im.resize((int(round(width * .75)), int(round(height * .75))))\n",
    "                im.save(\"temp.jpg\")\n",
    "                image_size = os.stat(\"temp.jpg\").st_size            \n",
    "            image_number = str(image[\"file_name\"] - 1000)\n",
    "            padded_number = '0' * (4 - len(image_number)) + image_number\n",
    "            #s3_client.copy(copy_source, target_bucket, str(volume[\"identifier\"]) + '-' + padded_number + \".jpg\", ExtraArgs={'ContentType': \"image/jpeg\", 'Metadata': {\"x-amz-meta-width\": str(image[\"width\"]), \"x-amz-meta-height\": str(image[\"height\"])}})\n",
    "            s3_client.upload_file(\"temp.jpg\", target_bucket, str(volume[\"identifier\"]) + '-' + padded_number + \".jpg\", ExtraArgs={'ContentType': \"image/jpeg\", 'Metadata': {\"width\": str(image[\"width\"]), \"height\": str(image[\"height\"])}})\n",
    "            images += 1\n",
    "            \n",
    "    os.remove(\"temp.jpg\")\n",
    "            \n",
    "    return str(images) + \" images copied from \" + source_bucket + \" to \" + target_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-situation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now working on 701236\n",
      "10579083\n",
      "1175750\n",
      "13419804\n",
      "1691496\n",
      "14241074\n",
      "1902434\n",
      "11893601\n",
      "1261265\n",
      "10828054\n",
      "1078452\n",
      "10960378\n",
      "1115240\n",
      "11835791\n",
      "1300910\n",
      "12164965\n",
      "1364123\n",
      "12077714\n",
      "1343849\n",
      "12362793\n",
      "1400880\n",
      "11633209\n",
      "1284579\n",
      "12162233\n",
      "1371021\n",
      "11832232\n",
      "1298795\n",
      "12083546\n",
      "1344742\n",
      "11618214\n",
      "1266248\n",
      "11669816\n",
      "1270145\n",
      "11662191\n",
      "1269467\n"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "copy_jpgs(\"brazil.json\", \"ssda-assets\", \"ssda-production-jpgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "entertaining-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_jpgs_for_prod(source_bucket_name, volume_id, target_bucket_name = \"ssda-production-jpgs\", source_prefix = \"\", copy_to_archive = False):\n",
    "    '''\n",
    "    Copies images of a single volume from one S3 bucket to another. Objects to be copied can be filtered by file type and/or key prefix.\n",
    "        source_bucket_name: name of S3 bucket to copy from\n",
    "        target_bucket_name: name of S3 bucket to copy to (specify if not ssda-production-jps)\n",
    "        volume_id: id of the volume in question\n",
    "        source_prefix (optional): S3 object key prefix to filter by\n",
    "        copy_to_archive: set to true to send a copy of each jpg to the archive bucket too\n",
    "        \n",
    "        returns: list of images with filenames and dimensions as well as a count of objects copied\n",
    "    '''\n",
    "    import boto3\n",
    "    import os\n",
    "    from PIL import Image\n",
    "    \n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_resource = boto3.resource(\"s3\")\n",
    "    source_bucket = s3_resource.Bucket(source_bucket_name)    \n",
    "    \n",
    "    image_count = 0\n",
    "    images = []   \n",
    "    \n",
    "    for obj in source_bucket.objects.filter(Prefix = source_prefix):\n",
    "        if obj.key.endswith(\"jpg\"):                     \n",
    "            s3_client.download_file(source_bucket_name, obj.key, \"temp.jpg\")            \n",
    "            image = {} \n",
    "            \n",
    "            with Image.open(\"temp.jpg\") as im:\n",
    "                width, height = im.size               \n",
    "                image_size = os.stat(\"temp.jpg\").st_size\n",
    "                while (image_size > 3000000):                   \n",
    "                    width, height = im.size\n",
    "                    im = im.resize((int(round(width * .75)), int(round(height * .75))))\n",
    "                    im.save(\"temp.jpg\")\n",
    "                    image_size = os.stat(\"temp.jpg\").st_size  \n",
    "                image[\"width\"] = width\n",
    "                image[\"height\"] = height            \n",
    "                    \n",
    "            image_number = str(image_count + 1)\n",
    "            padded_number = '0' * (4 - len(image_number)) + image_number\n",
    "            image[\"file name\"] = str(volume_id) + '-' + padded_number + \".jpg\"\n",
    "                        \n",
    "            s3_client.upload_file(\"temp.jpg\", target_bucket_name, image[\"file name\"], ExtraArgs={'ContentType': \"image/jpeg\", 'Metadata': {\"width\": str(image[\"width\"]), \"height\": str(image[\"height\"])}})\n",
    "            if copy_to_archive:\n",
    "                s3_client.upload_file(\"temp.jpg\", \"ssda-archive\", image[\"file name\"], ExtraArgs={'ContentType': \"image/jpeg\", 'Metadata': {\"width\": str(image[\"width\"]), \"height\": str(image[\"height\"])}})            \n",
    "            image_count += 1\n",
    "            images.append(image)\n",
    "            \n",
    "    os.remove(\"temp.jpg\")\n",
    "             \n",
    "    return images, image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "differential-bench",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now working on la-aurora/\n",
      "now working on la-aurora/IMG_1901.jpg\n",
      "now working on la-aurora/IMG_1902.jpg\n",
      "now working on la-aurora/IMG_1903.jpg\n",
      "now working on la-aurora/IMG_1904.jpg\n",
      "now working on la-aurora/IMG_1905.jpg\n",
      "now working on la-aurora/IMG_1906.jpg\n",
      "now working on la-aurora/IMG_1907.jpg\n",
      "now working on la-aurora/IMG_1908.jpg\n",
      "now working on la-aurora/IMG_1909.jpg\n",
      "now working on la-aurora/IMG_1910.jpg\n",
      "now working on la-aurora/IMG_1911.jpg\n",
      "now working on la-aurora/IMG_1912.jpg\n",
      "now working on la-aurora/IMG_1913.jpg\n",
      "now working on la-aurora/IMG_1914.jpg\n",
      "now working on la-aurora/IMG_1915.jpg\n",
      "now working on la-aurora/IMG_1916.jpg\n",
      "now working on la-aurora/IMG_1917.jpg\n",
      "now working on la-aurora/IMG_1918.jpg\n",
      "now working on la-aurora/IMG_1919.jpg\n",
      "now working on la-aurora/IMG_1920.jpg\n",
      "now working on la-aurora/IMG_1921.jpg\n",
      "now working on la-aurora/IMG_1922.jpg\n",
      "now working on la-aurora/IMG_1923.jpg\n",
      "now working on la-aurora/IMG_1924.jpg\n",
      "now working on la-aurora/IMG_1925.jpg\n",
      "now working on la-aurora/IMG_1926.jpg\n",
      "now working on la-aurora/IMG_1927.jpg\n",
      "now working on la-aurora/IMG_1928.jpg\n",
      "now working on la-aurora/IMG_1929.jpg\n",
      "now working on la-aurora/IMG_1930.jpg\n",
      "now working on la-aurora/IMG_1931.jpg\n",
      "now working on la-aurora/IMG_1932.jpg\n",
      "now working on la-aurora/IMG_1933.jpg\n",
      "now working on la-aurora/IMG_1934.jpg\n",
      "now working on la-aurora/IMG_1935.jpg\n",
      "now working on la-aurora/IMG_1936.jpg\n",
      "now working on la-aurora/IMG_1937.jpg\n",
      "now working on la-aurora/IMG_1938.jpg\n",
      "now working on la-aurora/IMG_1939.jpg\n",
      "now working on la-aurora/IMG_1940.jpg\n",
      "now working on la-aurora/IMG_1941.jpg\n",
      "now working on la-aurora/IMG_1942.jpg\n",
      "now working on la-aurora/IMG_1943.jpg\n",
      "now working on la-aurora/IMG_1944.jpg\n",
      "now working on la-aurora/IMG_1945.jpg\n",
      "now working on la-aurora/IMG_1946.jpg\n",
      "now working on la-aurora/IMG_1947.jpg\n",
      "now working on la-aurora/IMG_1948.jpg\n",
      "now working on la-aurora/IMG_1949.jpg\n",
      "now working on la-aurora/IMG_1950.jpg\n",
      "now working on la-aurora/IMG_1951.jpg\n",
      "now working on la-aurora/IMG_1952.jpg\n",
      "now working on la-aurora/IMG_1953.jpg\n",
      "now working on la-aurora/IMG_1954.jpg\n",
      "now working on la-aurora/IMG_1955.jpg\n",
      "now working on la-aurora/IMG_1956.jpg\n",
      "now working on la-aurora/IMG_1957.jpg\n",
      "now working on la-aurora/IMG_1958.jpg\n",
      "now working on la-aurora/IMG_1959.jpg\n",
      "now working on la-aurora/IMG_1960.jpg\n",
      "now working on la-aurora/IMG_1961.jpg\n",
      "now working on la-aurora/IMG_1962.jpg\n",
      "now working on la-aurora/IMG_1963.jpg\n",
      "now working on la-aurora/IMG_1964.jpg\n",
      "now working on la-aurora/IMG_1965.jpg\n",
      "now working on la-aurora/IMG_1966.jpg\n",
      "now working on la-aurora/IMG_1967.jpg\n",
      "now working on la-aurora/IMG_1968.jpg\n",
      "now working on la-aurora/IMG_1969.jpg\n",
      "now working on la-aurora/IMG_1970.jpg\n",
      "now working on la-aurora/IMG_1971.jpg\n",
      "now working on la-aurora/IMG_1972.jpg\n",
      "now working on la-aurora/IMG_1973.jpg\n",
      "now working on la-aurora/IMG_1974.jpg\n",
      "now working on la-aurora/IMG_1975.jpg\n",
      "now working on la-aurora/IMG_1976.jpg\n",
      "now working on la-aurora/IMG_1977.jpg\n",
      "now working on la-aurora/IMG_1978.jpg\n",
      "now working on la-aurora/IMG_1979.jpg\n",
      "now working on la-aurora/IMG_1980.jpg\n",
      "now working on la-aurora/IMG_1981.jpg\n",
      "now working on la-aurora/IMG_1982.jpg\n",
      "now working on la-aurora/IMG_1983.jpg\n",
      "now working on la-aurora/IMG_1984.jpg\n",
      "now working on la-aurora/IMG_1985.jpg\n",
      "now working on la-aurora/IMG_1986.jpg\n",
      "now working on la-aurora/IMG_1987.jpg\n",
      "now working on la-aurora/IMG_1988.jpg\n",
      "now working on la-aurora/IMG_1989.jpg\n",
      "now working on la-aurora/IMG_1990.jpg\n",
      "now working on la-aurora/IMG_1991.jpg\n",
      "now working on la-aurora/IMG_1992.jpg\n",
      "now working on la-aurora/IMG_1993.jpg\n",
      "now working on la-aurora/IMG_1994.jpg\n",
      "now working on la-aurora/IMG_1995.jpg\n",
      "now working on la-aurora/IMG_1996.jpg\n",
      "now working on la-aurora/IMG_1997.jpg\n",
      "now working on la-aurora/IMG_1998.jpg\n",
      "now working on la-aurora/IMG_1999.jpg\n",
      "now working on la-aurora/IMG_2000.jpg\n",
      "now working on la-aurora/IMG_2001.jpg\n",
      "now working on la-aurora/IMG_2002.jpg\n",
      "now working on la-aurora/IMG_2003.jpg\n",
      "now working on la-aurora/IMG_2004.jpg\n",
      "now working on la-aurora/IMG_2005.jpg\n",
      "now working on la-aurora/IMG_2006.jpg\n",
      "now working on la-aurora/IMG_2007.jpg\n",
      "now working on la-aurora/IMG_2008.jpg\n",
      "now working on la-aurora/IMG_2009.jpg\n",
      "now working on la-aurora/IMG_2010.jpg\n",
      "now working on la-aurora/IMG_2011.jpg\n",
      "now working on la-aurora/IMG_2012.jpg\n",
      "now working on la-aurora/IMG_2013.jpg\n",
      "now working on la-aurora/IMG_2014.jpg\n",
      "now working on la-aurora/IMG_2015.jpg\n",
      "now working on la-aurora/IMG_2016.jpg\n",
      "now working on la-aurora/IMG_2017.jpg\n",
      "now working on la-aurora/IMG_2018.jpg\n",
      "now working on la-aurora/IMG_2019.jpg\n",
      "now working on la-aurora/IMG_2020.jpg\n",
      "now working on la-aurora/IMG_2021.jpg\n",
      "now working on la-aurora/IMG_2022.jpg\n",
      "now working on la-aurora/IMG_2023.jpg\n",
      "now working on la-aurora/IMG_2024.jpg\n",
      "now working on la-aurora/IMG_2025.jpg\n",
      "now working on la-aurora/IMG_2026.jpg\n",
      "now working on la-aurora/IMG_2027.jpg\n",
      "now working on la-aurora/IMG_2028.jpg\n",
      "now working on la-aurora/IMG_2029.jpg\n",
      "now working on la-aurora/IMG_2030.jpg\n",
      "now working on la-aurora/IMG_2031.jpg\n",
      "now working on la-aurora/IMG_2032.jpg\n",
      "now working on la-aurora/IMG_2033.jpg\n",
      "now working on la-aurora/IMG_2034.jpg\n",
      "now working on la-aurora/IMG_2035.jpg\n",
      "now working on la-aurora/IMG_2036.jpg\n",
      "now working on la-aurora/IMG_2037.jpg\n",
      "now working on la-aurora/IMG_2038.jpg\n",
      "now working on la-aurora/IMG_2039.jpg\n",
      "now working on la-aurora/IMG_2040.jpg\n",
      "now working on la-aurora/IMG_2041.jpg\n",
      "now working on la-aurora/IMG_2042.jpg\n",
      "now working on la-aurora/IMG_2043.jpg\n",
      "now working on la-aurora/IMG_2044.jpg\n",
      "now working on la-aurora/IMG_2045.jpg\n",
      "now working on la-aurora/IMG_2046.jpg\n",
      "now working on la-aurora/IMG_2047.jpg\n",
      "now working on la-aurora/IMG_2048.jpg\n",
      "now working on la-aurora/IMG_2049.jpg\n",
      "now working on la-aurora/IMG_2050.jpg\n",
      "now working on la-aurora/IMG_2051.jpg\n",
      "now working on la-aurora/IMG_2052.jpg\n",
      "now working on la-aurora/IMG_2053.jpg\n",
      "now working on la-aurora/IMG_2054.jpg\n",
      "now working on la-aurora/IMG_2055.jpg\n",
      "now working on la-aurora/IMG_2056.jpg\n",
      "now working on la-aurora/IMG_2057.jpg\n",
      "now working on la-aurora/IMG_2058.jpg\n",
      "now working on la-aurora/IMG_2059.jpg\n",
      "now working on la-aurora/IMG_2060.jpg\n",
      "now working on la-aurora/IMG_2061.jpg\n",
      "now working on la-aurora/IMG_2062.jpg\n",
      "now working on la-aurora/IMG_2063.jpg\n",
      "now working on la-aurora/IMG_2064.jpg\n",
      "now working on la-aurora/IMG_2065.jpg\n",
      "now working on la-aurora/IMG_2066.jpg\n",
      "now working on la-aurora/IMG_2067.jpg\n",
      "now working on la-aurora/IMG_2068.jpg\n",
      "now working on la-aurora/IMG_2069.jpg\n",
      "now working on la-aurora/IMG_2070.jpg\n",
      "now working on la-aurora/IMG_2071.jpg\n",
      "now working on la-aurora/IMG_2072.jpg\n",
      "now working on la-aurora/IMG_2073.jpg\n",
      "now working on la-aurora/IMG_2074.jpg\n",
      "now working on la-aurora/IMG_2075.jpg\n",
      "now working on la-aurora/IMG_2076.jpg\n",
      "now working on la-aurora/IMG_2077.jpg\n",
      "now working on la-aurora/IMG_2078.jpg\n",
      "now working on la-aurora/IMG_2079.jpg\n",
      "now working on la-aurora/IMG_2080.jpg\n",
      "now working on la-aurora/IMG_2081.jpg\n",
      "now working on la-aurora/IMG_2082.jpg\n",
      "now working on la-aurora/IMG_2083.jpg\n",
      "now working on la-aurora/IMG_2084.jpg\n",
      "now working on la-aurora/IMG_2085.jpg\n",
      "now working on la-aurora/IMG_2086.jpg\n",
      "now working on la-aurora/IMG_2087.jpg\n",
      "now working on la-aurora/IMG_2088.jpg\n",
      "now working on la-aurora/IMG_2089.jpg\n",
      "now working on la-aurora/IMG_2090.jpg\n",
      "now working on la-aurora/IMG_2091.jpg\n",
      "now working on la-aurora/IMG_2092.jpg\n",
      "now working on la-aurora/IMG_2093.jpg\n",
      "now working on la-aurora/IMG_2094.jpg\n",
      "now working on la-aurora/IMG_2095.jpg\n",
      "now working on la-aurora/IMG_2096.jpg\n",
      "now working on la-aurora/IMG_2097.jpg\n",
      "now working on la-aurora/IMG_2098.jpg\n",
      "now working on la-aurora/IMG_2099.jpg\n",
      "now working on la-aurora/IMG_2100.jpg\n",
      "now working on la-aurora/IMG_2101.jpg\n",
      "now working on la-aurora/IMG_2102.jpg\n",
      "now working on la-aurora/IMG_2103.jpg\n",
      "now working on la-aurora/IMG_2104.jpg\n",
      "now working on la-aurora/IMG_2105.jpg\n",
      "now working on la-aurora/IMG_2106.jpg\n",
      "now working on la-aurora/IMG_2107.jpg\n",
      "now working on la-aurora/IMG_2108.jpg\n",
      "now working on la-aurora/IMG_2109.jpg\n",
      "now working on la-aurora/IMG_2110.jpg\n",
      "now working on la-aurora/IMG_2111.jpg\n",
      "now working on la-aurora/IMG_2112.jpg\n",
      "now working on la-aurora/IMG_2113.jpg\n",
      "now working on la-aurora/IMG_2114.jpg\n",
      "now working on la-aurora/IMG_2115.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now working on la-aurora/IMG_2116.jpg\n",
      "now working on la-aurora/IMG_2117.jpg\n",
      "now working on la-aurora/IMG_2118.jpg\n",
      "now working on la-aurora/IMG_2119.jpg\n",
      "now working on la-aurora/IMG_2120.jpg\n",
      "now working on la-aurora/IMG_2121.jpg\n",
      "now working on la-aurora/IMG_2122.jpg\n",
      "now working on la-aurora/IMG_2123.jpg\n",
      "now working on la-aurora/IMG_2124.jpg\n",
      "now working on la-aurora/IMG_2125.jpg\n",
      "now working on la-aurora/IMG_2126.jpg\n",
      "now working on la-aurora/IMG_2127.jpg\n",
      "now working on la-aurora/IMG_2128.jpg\n",
      "now working on la-aurora/IMG_2129.jpg\n",
      "now working on la-aurora/IMG_2130.jpg\n",
      "now working on la-aurora/IMG_2131.jpg\n",
      "now working on la-aurora/IMG_2132.jpg\n",
      "now working on la-aurora/IMG_2133.jpg\n",
      "now working on la-aurora/IMG_2134.jpg\n",
      "now working on la-aurora/IMG_2135.jpg\n",
      "now working on la-aurora/IMG_2136.jpg\n",
      "now working on la-aurora/IMG_2137.jpg\n",
      "now working on la-aurora/IMG_2138.jpg\n",
      "now working on la-aurora/IMG_2139.jpg\n",
      "now working on la-aurora/IMG_2140.jpg\n",
      "now working on la-aurora/IMG_2141.jpg\n",
      "now working on la-aurora/IMG_2142.jpg\n",
      "now working on la-aurora/IMG_2143.jpg\n",
      "now working on la-aurora/IMG_2144.jpg\n",
      "now working on la-aurora/IMG_2145.jpg\n",
      "now working on la-aurora/IMG_2146.jpg\n",
      "now working on la-aurora/IMG_2147.jpg\n",
      "now working on la-aurora/IMG_2148.jpg\n",
      "now working on la-aurora/IMG_2149.jpg\n",
      "now working on la-aurora/IMG_2150.jpg\n",
      "now working on la-aurora/IMG_2151.jpg\n",
      "now working on la-aurora/IMG_2152.jpg\n",
      "now working on la-aurora/IMG_2153.jpg\n",
      "now working on la-aurora/IMG_2154.jpg\n",
      "now working on la-aurora/IMG_2155.jpg\n",
      "now working on la-aurora/IMG_2156.jpg\n",
      "now working on la-aurora/IMG_2157.jpg\n",
      "now working on la-aurora/IMG_2158.jpg\n",
      "now working on la-aurora/IMG_2159.jpg\n",
      "now working on la-aurora/IMG_2160.jpg\n",
      "now working on la-aurora/IMG_2161.jpg\n",
      "now working on la-aurora/IMG_2162.jpg\n",
      "now working on la-aurora/IMG_2163.jpg\n",
      "now working on la-aurora/IMG_2164.jpg\n",
      "now working on la-aurora/IMG_2165.jpg\n",
      "now working on la-aurora/IMG_2166.jpg\n",
      "now working on la-aurora/IMG_2167.jpg\n",
      "now working on la-aurora/IMG_2168.jpg\n",
      "now working on la-aurora/IMG_2169.jpg\n",
      "now working on la-aurora/IMG_2170.jpg\n",
      "now working on la-aurora/IMG_2171.jpg\n",
      "now working on la-aurora/IMG_2172.jpg\n",
      "now working on la-aurora/IMG_2173.jpg\n",
      "now working on la-aurora/IMG_2174.jpg\n",
      "now working on la-aurora/IMG_2175.jpg\n",
      "now working on la-aurora/IMG_2176.jpg\n",
      "now working on la-aurora/IMG_2177.jpg\n",
      "now working on la-aurora/IMG_2178.jpg\n",
      "now working on la-aurora/IMG_2179.jpg\n",
      "now working on la-aurora/IMG_2180.jpg\n",
      "now working on la-aurora/IMG_2181.jpg\n",
      "now working on la-aurora/IMG_2182.jpg\n",
      "now working on la-aurora/IMG_2183.jpg\n",
      "now working on la-aurora/IMG_2184.jpg\n",
      "now working on la-aurora/IMG_2185.jpg\n",
      "now working on la-aurora/IMG_2186.jpg\n",
      "now working on la-aurora/IMG_2187.jpg\n",
      "now working on la-aurora/IMG_2188.jpg\n",
      "now working on la-aurora/IMG_2189.jpg\n",
      "now working on la-aurora/IMG_2190.jpg\n",
      "now working on la-aurora/IMG_2191.jpg\n",
      "now working on la-aurora/IMG_2192.jpg\n",
      "now working on la-aurora/IMG_2193.jpg\n",
      "now working on la-aurora/IMG_2194.jpg\n",
      "now working on la-aurora/IMG_2195.jpg\n",
      "now working on la-aurora/IMG_2196.jpg\n",
      "now working on la-aurora/IMG_2197.jpg\n",
      "now working on la-aurora/IMG_2198.jpg\n",
      "now working on la-aurora/IMG_2199.jpg\n",
      "now working on la-aurora/IMG_2200.jpg\n",
      "now working on la-aurora/IMG_2201.jpg\n",
      "now working on la-aurora/IMG_2202.jpg\n",
      "now working on la-aurora/IMG_2203.jpg\n",
      "now working on la-aurora/IMG_2204.jpg\n",
      "now working on la-aurora/IMG_2205.jpg\n",
      "now working on la-aurora/IMG_2206.jpg\n",
      "now working on la-aurora/IMG_2207.jpg\n",
      "now working on la-aurora/IMG_2208.jpg\n",
      "now working on la-aurora/IMG_2209.jpg\n",
      "now working on la-aurora/IMG_2210.jpg\n",
      "now working on la-aurora/IMG_2211.jpg\n",
      "now working on la-aurora/IMG_2212.jpg\n",
      "now working on la-aurora/IMG_2213.jpg\n",
      "now working on la-aurora/IMG_2214.jpg\n",
      "now working on la-aurora/IMG_2215.jpg\n",
      "now working on la-aurora/IMG_2216.jpg\n",
      "now working on la-aurora/IMG_2217.jpg\n",
      "now working on la-aurora/IMG_2218.jpg\n",
      "now working on la-aurora/IMG_2219.jpg\n",
      "now working on la-aurora/IMG_2220.jpg\n",
      "now working on la-aurora/IMG_2221.jpg\n",
      "now working on la-aurora/IMG_2222.jpg\n",
      "now working on la-aurora/IMG_2223.jpg\n",
      "now working on la-aurora/IMG_2224.jpg\n",
      "now working on la-aurora/IMG_2225.jpg\n",
      "now working on la-aurora/IMG_2226.jpg\n",
      "now working on la-aurora/IMG_2227.jpg\n",
      "now working on la-aurora/IMG_2228.jpg\n",
      "now working on la-aurora/IMG_2229.jpg\n",
      "now working on la-aurora/IMG_2230.jpg\n",
      "now working on la-aurora/IMG_2231.jpg\n",
      "now working on la-aurora/IMG_2232.jpg\n",
      "now working on la-aurora/IMG_2233.jpg\n",
      "now working on la-aurora/IMG_2234.jpg\n",
      "now working on la-aurora/IMG_2235.jpg\n",
      "now working on la-aurora/IMG_2236.jpg\n",
      "now working on la-aurora/IMG_2237.jpg\n",
      "now working on la-aurora/IMG_2238.jpg\n",
      "now working on la-aurora/IMG_2239.jpg\n",
      "now working on la-aurora/IMG_2240.jpg\n",
      "now working on la-aurora/IMG_2241.jpg\n",
      "now working on la-aurora/IMG_2242.jpg\n",
      "now working on la-aurora/IMG_2243.jpg\n",
      "now working on la-aurora/IMG_2244.jpg\n",
      "now working on la-aurora/IMG_2245.jpg\n",
      "now working on la-aurora/IMG_2246.jpg\n",
      "now working on la-aurora/IMG_2247.jpg\n",
      "now working on la-aurora/IMG_2248.jpg\n",
      "now working on la-aurora/IMG_2249.jpg\n",
      "now working on la-aurora/IMG_2250.jpg\n",
      "now working on la-aurora/IMG_2251.jpg\n",
      "now working on la-aurora/IMG_2252.jpg\n",
      "now working on la-aurora/IMG_2253.jpg\n",
      "now working on la-aurora/IMG_2254.jpg\n",
      "now working on la-aurora/IMG_2255.jpg\n",
      "now working on la-aurora/IMG_2256.jpg\n",
      "now working on la-aurora/IMG_2257.jpg\n",
      "now working on la-aurora/IMG_2258.jpg\n",
      "now working on la-aurora/IMG_2259.jpg\n",
      "now working on la-aurora/IMG_2260.jpg\n",
      "now working on la-aurora/IMG_2261.jpg\n",
      "now working on la-aurora/IMG_2262.jpg\n",
      "now working on la-aurora/IMG_2263.jpg\n",
      "now working on la-aurora/IMG_2264.jpg\n",
      "now working on la-aurora/IMG_2265.jpg\n",
      "now working on la-aurora/IMG_2266.jpg\n",
      "now working on la-aurora/IMG_2267.jpg\n",
      "now working on la-aurora/IMG_2268.jpg\n",
      "now working on la-aurora/IMG_2269.jpg\n",
      "now working on la-aurora/IMG_2270.jpg\n",
      "now working on la-aurora/IMG_2271.jpg\n",
      "now working on la-aurora/IMG_2272.jpg\n",
      "now working on la-aurora/IMG_2273.jpg\n",
      "now working on la-aurora/IMG_2274.jpg\n",
      "now working on la-aurora/IMG_2275.jpg\n",
      "now working on la-aurora/IMG_2276.jpg\n",
      "now working on la-aurora/IMG_2277.jpg\n",
      "now working on la-aurora/IMG_2278.jpg\n",
      "now working on la-aurora/IMG_2279.jpg\n",
      "now working on la-aurora/IMG_2280.jpg\n",
      "now working on la-aurora/IMG_2281.jpg\n",
      "now working on la-aurora/IMG_2282.jpg\n",
      "now working on la-aurora/IMG_2283.jpg\n",
      "now working on la-aurora/IMG_2284.jpg\n",
      "now working on la-aurora/IMG_2285.jpg\n",
      "now working on la-aurora/IMG_2286.jpg\n",
      "now working on la-aurora/IMG_2287.jpg\n",
      "now working on la-aurora/IMG_2288.jpg\n",
      "now working on la-aurora/IMG_2289.jpg\n",
      "now working on la-aurora/IMG_2290.jpg\n",
      "now working on la-aurora/IMG_2291.jpg\n",
      "now working on la-aurora/IMG_2292.jpg\n",
      "now working on la-aurora/IMG_2293.jpg\n",
      "now working on la-aurora/IMG_2294.jpg\n",
      "now working on la-aurora/IMG_2295.jpg\n",
      "now working on la-aurora/IMG_2296.jpg\n",
      "now working on la-aurora/IMG_2297.jpg\n",
      "now working on la-aurora/IMG_2298.jpg\n",
      "now working on la-aurora/IMG_2299.jpg\n",
      "now working on la-aurora/IMG_2300.jpg\n",
      "now working on la-aurora/IMG_2301.jpg\n",
      "now working on la-aurora/IMG_2302.jpg\n",
      "now working on la-aurora/IMG_2303.jpg\n",
      "now working on la-aurora/IMG_2304.jpg\n",
      "now working on la-aurora/IMG_2305.jpg\n",
      "now working on la-aurora/IMG_2306.jpg\n",
      "now working on la-aurora/IMG_2307.jpg\n",
      "now working on la-aurora/IMG_2308.jpg\n",
      "now working on la-aurora/IMG_2309.jpg\n",
      "now working on la-aurora/IMG_2310.jpg\n",
      "now working on la-aurora/IMG_2311.jpg\n",
      "now working on la-aurora/IMG_2312.jpg\n",
      "now working on la-aurora/IMG_2313.jpg\n",
      "now working on la-aurora/IMG_2314.jpg\n",
      "now working on la-aurora/IMG_2315.jpg\n",
      "now working on la-aurora/IMG_2316.jpg\n",
      "now working on la-aurora/IMG_2317.jpg\n",
      "now working on la-aurora/IMG_2318.jpg\n",
      "now working on la-aurora/IMG_2319.jpg\n",
      "now working on la-aurora/IMG_2320.jpg\n",
      "now working on la-aurora/IMG_2321.jpg\n",
      "now working on la-aurora/IMG_2322.jpg\n",
      "now working on la-aurora/IMG_2323.jpg\n",
      "now working on la-aurora/IMG_2324.jpg\n",
      "now working on la-aurora/IMG_2325.jpg\n",
      "now working on la-aurora/IMG_2326.jpg\n",
      "now working on la-aurora/IMG_2327.jpg\n",
      "now working on la-aurora/IMG_2328.jpg\n",
      "now working on la-aurora/IMG_2329.jpg\n",
      "now working on la-aurora/IMG_2330.jpg\n",
      "now working on la-aurora/IMG_2331.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now working on la-aurora/IMG_2332.jpg\n",
      "now working on la-aurora/IMG_2333.jpg\n",
      "now working on la-aurora/IMG_2334.jpg\n",
      "now working on la-aurora/IMG_2335.jpg\n",
      "now working on la-aurora/IMG_2336.jpg\n",
      "now working on la-aurora/IMG_2337.jpg\n",
      "now working on la-aurora/IMG_2338.jpg\n",
      "now working on la-aurora/IMG_2339.jpg\n",
      "now working on la-aurora/IMG_2340.jpg\n",
      "now working on la-aurora/IMG_2341.jpg\n",
      "now working on la-aurora/IMG_2342.jpg\n",
      "now working on la-aurora/IMG_2343.jpg\n",
      "now working on la-aurora/IMG_2344.jpg\n",
      "now working on la-aurora/IMG_2345.jpg\n",
      "now working on la-aurora/IMG_2346.jpg\n",
      "now working on la-aurora/IMG_2347.jpg\n",
      "now working on la-aurora/IMG_2348.jpg\n",
      "now working on la-aurora/IMG_2349.jpg\n",
      "now working on la-aurora/IMG_2350.jpg\n",
      "now working on la-aurora/IMG_2351.jpg\n",
      "now working on la-aurora/IMG_2352.jpg\n",
      "now working on la-aurora/IMG_2353.jpg\n",
      "now working on la-aurora/IMG_2354.jpg\n",
      "now working on la-aurora/IMG_2355.jpg\n",
      "now working on la-aurora/IMG_2356.jpg\n",
      "now working on la-aurora/IMG_2357.jpg\n",
      "now working on la-aurora/IMG_2358.jpg\n",
      "now working on la-aurora/IMG_2359.jpg\n",
      "now working on la-aurora/IMG_2360.jpg\n",
      "now working on la-aurora/IMG_2361.jpg\n",
      "now working on la-aurora/IMG_2362.jpg\n",
      "now working on la-aurora/IMG_2363.jpg\n",
      "now working on la-aurora/IMG_2364.jpg\n",
      "now working on la-aurora/IMG_2365.jpg\n",
      "now working on la-aurora/IMG_2366.jpg\n",
      "now working on la-aurora/IMG_2367.jpg\n",
      "now working on la-aurora/IMG_2368.jpg\n",
      "now working on la-aurora/IMG_2369.jpg\n",
      "now working on la-aurora/IMG_2370.jpg\n",
      "now working on la-aurora/IMG_2371.jpg\n",
      "now working on la-aurora/IMG_2372.jpg\n",
      "now working on la-aurora/IMG_2373.jpg\n",
      "now working on la-aurora/IMG_2374.jpg\n",
      "now working on la-aurora/IMG_2375.jpg\n",
      "now working on la-aurora/IMG_2376.jpg\n",
      "now working on la-aurora/IMG_2377.jpg\n",
      "now working on la-aurora/IMG_2378.jpg\n",
      "now working on la-aurora/IMG_2379.jpg\n",
      "now working on la-aurora/IMG_2380.jpg\n",
      "now working on la-aurora/IMG_2381.jpg\n",
      "now working on la-aurora/IMG_2382.jpg\n",
      "now working on la-aurora/IMG_2383.jpg\n",
      "now working on la-aurora/IMG_2384.jpg\n",
      "now working on la-aurora/IMG_2385.jpg\n",
      "now working on la-aurora/IMG_2386.jpg\n",
      "now working on la-aurora/IMG_2387.jpg\n",
      "now working on la-aurora/IMG_2388.jpg\n",
      "now working on la-aurora/IMG_2389.jpg\n",
      "now working on la-aurora/IMG_2390.jpg\n",
      "now working on la-aurora/IMG_2391.jpg\n",
      "now working on la-aurora/IMG_2392.jpg\n",
      "now working on la-aurora/IMG_2393.jpg\n",
      "now working on la-aurora/IMG_2394.jpg\n",
      "now working on la-aurora/IMG_2395.jpg\n",
      "now working on la-aurora/IMG_2396.jpg\n",
      "now working on la-aurora/IMG_2397.jpg\n",
      "now working on la-aurora/IMG_2398.jpg\n",
      "now working on la-aurora/IMG_2399.jpg\n",
      "now working on la-aurora/IMG_2400.jpg\n",
      "now working on la-aurora/IMG_2401.jpg\n",
      "now working on la-aurora/IMG_2402.jpg\n",
      "now working on la-aurora/IMG_2403.jpg\n",
      "now working on la-aurora/IMG_2404.jpg\n",
      "now working on la-aurora/IMG_2405.jpg\n",
      "now working on la-aurora/IMG_2406.jpg\n",
      "now working on la-aurora/IMG_2407.jpg\n",
      "now working on la-aurora/IMG_2408.jpg\n",
      "now working on la-aurora/IMG_2409.jpg\n",
      "now working on la-aurora/IMG_2410.jpg\n",
      "now working on la-aurora/IMG_2411.jpg\n",
      "now working on la-aurora/IMG_2412.jpg\n",
      "now working on la-aurora/IMG_2413.jpg\n",
      "now working on la-aurora/IMG_2414.jpg\n",
      "now working on la-aurora/IMG_2415.jpg\n",
      "now working on la-aurora/IMG_2416.jpg\n",
      "now working on la-aurora/IMG_2417.jpg\n",
      "now working on la-aurora/IMG_2418.jpg\n",
      "now working on la-aurora/IMG_2419.jpg\n",
      "now working on la-aurora/IMG_2420.jpg\n",
      "now working on la-aurora/IMG_2421.jpg\n",
      "now working on la-aurora/IMG_2422.jpg\n",
      "now working on la-aurora/IMG_2423.jpg\n",
      "now working on la-aurora/IMG_2424.jpg\n",
      "now working on la-aurora/IMG_2425.jpg\n",
      "now working on la-aurora/IMG_2426.jpg\n",
      "now working on la-aurora/IMG_2427.jpg\n",
      "now working on la-aurora/IMG_2428.jpg\n",
      "now working on la-aurora/IMG_2429.jpg\n",
      "now working on la-aurora/IMG_2430.jpg\n",
      "now working on la-aurora/IMG_2431.jpg\n",
      "now working on la-aurora/IMG_2432.jpg\n",
      "now working on la-aurora/IMG_2433.jpg\n",
      "now working on la-aurora/IMG_2434.jpg\n",
      "now working on la-aurora/IMG_2435.jpg\n",
      "now working on la-aurora/IMG_2436.jpg\n",
      "now working on la-aurora/IMG_2437.jpg\n",
      "now working on la-aurora/IMG_2438.jpg\n",
      "now working on la-aurora/IMG_2439.jpg\n",
      "now working on la-aurora/IMG_2440.jpg\n",
      "now working on la-aurora/IMG_2441.jpg\n",
      "now working on la-aurora/IMG_2442.jpg\n",
      "now working on la-aurora/IMG_2443.jpg\n",
      "now working on la-aurora/IMG_2444.jpg\n",
      "now working on la-aurora/IMG_2445.jpg\n",
      "now working on la-aurora/IMG_2446.jpg\n",
      "now working on la-aurora/IMG_2447.jpg\n",
      "now working on la-aurora/IMG_2448.jpg\n",
      "now working on la-aurora/IMG_2449.jpg\n",
      "now working on la-aurora/IMG_2450.jpg\n",
      "now working on la-aurora/IMG_2451.jpg\n",
      "now working on la-aurora/IMG_2452.jpg\n",
      "now working on la-aurora/IMG_2453.jpg\n",
      "now working on la-aurora/IMG_2454.jpg\n",
      "now working on la-aurora/IMG_2455.jpg\n",
      "now working on la-aurora/IMG_2456.jpg\n",
      "now working on la-aurora/IMG_2457.jpg\n",
      "now working on la-aurora/IMG_2458.jpg\n",
      "now working on la-aurora/IMG_2459.jpg\n",
      "now working on la-aurora/IMG_2460.jpg\n",
      "now working on la-aurora/IMG_2461.jpg\n",
      "now working on la-aurora/IMG_2462.jpg\n",
      "now working on la-aurora/IMG_2463.jpg\n",
      "now working on la-aurora/IMG_2464.jpg\n",
      "now working on la-aurora/IMG_2465.jpg\n",
      "now working on la-aurora/IMG_2466.jpg\n",
      "now working on la-aurora/IMG_2467.jpg\n",
      "now working on la-aurora/IMG_2468.jpg\n",
      "now working on la-aurora/IMG_2469.jpg\n",
      "now working on la-aurora/IMG_2470.jpg\n",
      "now working on la-aurora/IMG_2471.jpg\n",
      "now working on la-aurora/IMG_2472.jpg\n",
      "now working on la-aurora/IMG_2473.jpg\n",
      "now working on la-aurora/IMG_2474.jpg\n",
      "now working on la-aurora/IMG_2475.jpg\n",
      "now working on la-aurora/IMG_2476.jpg\n",
      "now working on la-aurora/IMG_2477.jpg\n",
      "now working on la-aurora/IMG_2478.jpg\n",
      "now working on la-aurora/IMG_2479.jpg\n",
      "now working on la-aurora/IMG_2480.jpg\n",
      "now working on la-aurora/IMG_2481.jpg\n",
      "now working on la-aurora/IMG_2482.jpg\n",
      "now working on la-aurora/IMG_2483.jpg\n",
      "now working on la-aurora/IMG_2484.jpg\n",
      "now working on la-aurora/IMG_2485.jpg\n",
      "now working on la-aurora/IMG_2486.jpg\n",
      "now working on la-aurora/IMG_2487.jpg\n",
      "now working on la-aurora/IMG_2488.jpg\n",
      "now working on la-aurora/IMG_2489.jpg\n",
      "now working on la-aurora/IMG_2490.jpg\n",
      "now working on la-aurora/IMG_2491.jpg\n",
      "now working on la-aurora/IMG_2492.jpg\n",
      "now working on la-aurora/IMG_2493.jpg\n",
      "now working on la-aurora/IMG_2494.jpg\n",
      "now working on la-aurora/IMG_2495.jpg\n",
      "now working on la-aurora/IMG_2496.jpg\n",
      "now working on la-aurora/IMG_2497.jpg\n",
      "now working on la-aurora/IMG_2498.jpg\n",
      "now working on la-aurora/IMG_2499.jpg\n",
      "now working on la-aurora/IMG_2500.jpg\n",
      "now working on la-aurora/IMG_2501.jpg\n",
      "now working on la-aurora/IMG_2502.jpg\n",
      "now working on la-aurora/IMG_2503.jpg\n",
      "now working on la-aurora/IMG_2504.jpg\n",
      "now working on la-aurora/IMG_2505.jpg\n",
      "now working on la-aurora/IMG_2506.jpg\n",
      "now working on la-aurora/IMG_2507.jpg\n",
      "now working on la-aurora/IMG_2508.jpg\n",
      "now working on la-aurora/IMG_2509.jpg\n",
      "now working on la-aurora/IMG_2510.jpg\n",
      "now working on la-aurora/IMG_2511.jpg\n",
      "now working on la-aurora/IMG_2512.jpg\n",
      "now working on la-aurora/IMG_2513.jpg\n",
      "now working on la-aurora/IMG_2514.jpg\n",
      "now working on la-aurora/IMG_2515.jpg\n",
      "now working on la-aurora/IMG_2516.jpg\n",
      "now working on la-aurora/IMG_2517.jpg\n",
      "now working on la-aurora/IMG_2518.jpg\n",
      "now working on la-aurora/IMG_2519.jpg\n",
      "now working on la-aurora/IMG_2520.jpg\n",
      "now working on la-aurora/IMG_2521.jpg\n",
      "now working on la-aurora/IMG_2522.jpg\n",
      "now working on la-aurora/IMG_2523.jpg\n",
      "now working on la-aurora/IMG_2524.jpg\n",
      "now working on la-aurora/IMG_2525.jpg\n",
      "now working on la-aurora/IMG_2526.jpg\n",
      "now working on la-aurora/IMG_2527.jpg\n",
      "now working on la-aurora/IMG_2528.jpg\n",
      "now working on la-aurora/IMG_2529.jpg\n",
      "now working on la-aurora/IMG_2530.jpg\n",
      "now working on la-aurora/IMG_2531.jpg\n",
      "now working on la-aurora/IMG_2532.jpg\n",
      "now working on la-aurora/IMG_2533.jpg\n",
      "now working on la-aurora/IMG_2534.jpg\n",
      "now working on la-aurora/IMG_2535.jpg\n",
      "now working on la-aurora/IMG_2536.jpg\n",
      "now working on la-aurora/IMG_2537.jpg\n",
      "now working on la-aurora/IMG_2538.jpg\n",
      "now working on la-aurora/IMG_2539.jpg\n",
      "now working on la-aurora/IMG_2540.jpg\n",
      "now working on la-aurora/IMG_2541.jpg\n",
      "now working on la-aurora/IMG_2542.jpg\n",
      "now working on la-aurora/IMG_2543.jpg\n",
      "now working on la-aurora/IMG_2544.jpg\n",
      "now working on la-aurora/IMG_2545.jpg\n",
      "now working on la-aurora/IMG_2546.jpg\n",
      "now working on la-aurora/IMG_2547.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now working on la-aurora/IMG_2548.jpg\n",
      "now working on la-aurora/IMG_2549.jpg\n",
      "now working on la-aurora/IMG_2550.jpg\n",
      "now working on la-aurora/IMG_2551.jpg\n",
      "now working on la-aurora/IMG_2552.jpg\n",
      "now working on la-aurora/IMG_2553.jpg\n",
      "now working on la-aurora/IMG_2554.jpg\n",
      "now working on la-aurora/IMG_2555.jpg\n",
      "now working on la-aurora/IMG_2556.jpg\n",
      "now working on la-aurora/IMG_2557.jpg\n",
      "now working on la-aurora/IMG_2558.jpg\n",
      "now working on la-aurora/IMG_2559.jpg\n",
      "now working on la-aurora/IMG_2560.jpg\n",
      "now working on la-aurora/IMG_2561.jpg\n",
      "now working on la-aurora/IMG_2562.jpg\n",
      "now working on la-aurora/IMG_2563.jpg\n",
      "now working on la-aurora/IMG_2564.jpg\n",
      "now working on la-aurora/IMG_2565.jpg\n",
      "now working on la-aurora/IMG_2566.jpg\n",
      "now working on la-aurora/IMG_2567.jpg\n",
      "now working on la-aurora/IMG_2568.jpg\n",
      "now working on la-aurora/IMG_2569.jpg\n",
      "now working on la-aurora/IMG_2570.jpg\n",
      "now working on la-aurora/IMG_2571.jpg\n",
      "now working on la-aurora/IMG_2572.jpg\n",
      "now working on la-aurora/IMG_2573.jpg\n",
      "now working on la-aurora/IMG_2574.jpg\n",
      "now working on la-aurora/IMG_2575.jpg\n",
      "now working on la-aurora/IMG_2576.jpg\n",
      "now working on la-aurora/IMG_2577.jpg\n",
      "now working on la-aurora/IMG_2578.jpg\n",
      "now working on la-aurora/IMG_2579.jpg\n",
      "now working on la-aurora/IMG_2580.jpg\n",
      "now working on la-aurora/IMG_2581.jpg\n",
      "now working on la-aurora/IMG_2582.jpg\n",
      "now working on la-aurora/IMG_2583.jpg\n",
      "now working on la-aurora/IMG_2584.jpg\n",
      "now working on la-aurora/IMG_2585.jpg\n",
      "now working on la-aurora/IMG_2586.jpg\n",
      "now working on la-aurora/IMG_2587.jpg\n",
      "now working on la-aurora/IMG_2588.jpg\n",
      "now working on la-aurora/IMG_2589.jpg\n",
      "now working on la-aurora/IMG_2590.jpg\n",
      "now working on la-aurora/IMG_2591.jpg\n",
      "now working on la-aurora/IMG_2592.jpg\n",
      "now working on la-aurora/IMG_2593.jpg\n",
      "now working on la-aurora/IMG_2594.jpg\n",
      "now working on la-aurora/IMG_2595.jpg\n",
      "now working on la-aurora/IMG_2596.jpg\n",
      "now working on la-aurora/IMG_2597.jpg\n",
      "now working on la-aurora/IMG_2598.jpg\n",
      "now working on la-aurora/IMG_2599.jpg\n",
      "now working on la-aurora/IMG_2600.jpg\n",
      "now working on la-aurora/IMG_2601.jpg\n",
      "now working on la-aurora/IMG_2602.jpg\n",
      "now working on la-aurora/IMG_2603.jpg\n",
      "now working on la-aurora/IMG_2604.jpg\n",
      "now working on la-aurora/IMG_2605.jpg\n",
      "now working on la-aurora/IMG_2606.jpg\n",
      "now working on la-aurora/IMG_2607.jpg\n",
      "now working on la-aurora/IMG_2608.jpg\n",
      "now working on la-aurora/IMG_2609.jpg\n",
      "now working on la-aurora/IMG_2610.jpg\n",
      "now working on la-aurora/IMG_2611.jpg\n",
      "now working on la-aurora/IMG_2612.jpg\n",
      "now working on la-aurora/IMG_2613.jpg\n",
      "now working on la-aurora/IMG_2614.jpg\n",
      "now working on la-aurora/IMG_2615.jpg\n",
      "now working on la-aurora/IMG_2616.jpg\n",
      "now working on la-aurora/IMG_2617.jpg\n",
      "now working on la-aurora/IMG_2618.jpg\n",
      "now working on la-aurora/IMG_2619.jpg\n",
      "now working on la-aurora/IMG_2620.jpg\n",
      "now working on la-aurora/IMG_2621.jpg\n",
      "now working on la-aurora/IMG_2622.jpg\n",
      "now working on la-aurora/IMG_2623.jpg\n"
     ]
    }
   ],
   "source": [
    "images, count = copy_jpgs_for_prod(\"ssda-misc\", 702001, source_prefix = \"la-aurora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unsigned-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def build_volume_records(json_path, target_bucket):\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_client = boto3.client('s3')\n",
    "    #bucket = s3_resource.Bucket(target_bucket)    \n",
    "    \n",
    "    volumes = 0\n",
    "    \n",
    "    with open(json_path, encoding=\"utf-8\") as jsonfile:\n",
    "        data = json.load(jsonfile)\n",
    "        \n",
    "    for volume in data[\"volumes\"]:\n",
    "        keys_to_delete = [\"s3_path\", \"contributor\"]\n",
    "        keys_to_add = []\n",
    "        values_to_add = []\n",
    "        for key in volume:\n",
    "            if key in keys_to_delete:                \n",
    "                continue\n",
    "            elif (type(volume[key]) == list) and ((key == \"creator\") or (key == \"format\")):                \n",
    "                temp = ''\n",
    "                for element in volume[key]:\n",
    "                    temp += element + \"; \"\n",
    "                temp = temp[:len(temp) - 2]\n",
    "                volume[key] = temp\n",
    "            elif key == \"date\":\n",
    "                if volume[key][\"start year\"] == None:\n",
    "                    start_date = \"1500\"\n",
    "                else:\n",
    "                    start_date = str(volume[key][\"start year\"])\n",
    "                if volume[key][\"start month\"] != None:\n",
    "                    str_start_month = str(volume[key][\"start month\"])\n",
    "                    start_date += '-' + '0' * (2 - len(str_start_month)) + str_start_month\n",
    "                    if volume[key][\"start day\"] != None:\n",
    "                        str_start_day = str(volume[key][\"start day\"])\n",
    "                        start_date += '-' + '0' * (2 - len(str_start_day)) + str_start_day\n",
    "                if volume[key][\"end year\"] == None:\n",
    "                    end_date = \"2000\"\n",
    "                else:\n",
    "                    end_date = str(volume[key][\"end year\"])\n",
    "                if volume[key][\"end month\"] != None:\n",
    "                    str_end_month = str(volume[key][\"end month\"])\n",
    "                    end_date += '-' + '0' * (2 - len(str_end_month)) + str_end_month\n",
    "                    if volume[key][\"end day\"] != None:\n",
    "                        str_end_day = str(volume[key][\"end day\"])\n",
    "                        end_date += '-' + '0' * (2 - len(str_end_day)) + str_end_day                \n",
    "                keys_to_delete.append(key)                \n",
    "            elif type(volume[key]) == dict:\n",
    "                for key_key in volume[key]:\n",
    "                    keys_to_add.append(key_key)\n",
    "                    values_to_add.append(volume[key][key_key])\n",
    "                keys_to_delete.append(key)\n",
    "            elif volume[key] == None:\n",
    "                volume[key] = ''\n",
    "                \n",
    "        for i in range(len(keys_to_add)):\n",
    "            if values_to_add[i] == None:\n",
    "                values_to_add[i] = ''\n",
    "            \n",
    "            if (keys_to_add[i] == \"coords\") and (values_to_add[i] != ''):\n",
    "                coords = values_to_add[i].split(',')\n",
    "                volume[keys_to_add[i]] = '(' + coords[0] + \", \" + coords[1] + ')'\n",
    "                continue\n",
    "            elif keys_to_add[i] == \"coords\":\n",
    "                volume[keys_to_add[i]] = \"(0,0)\"\n",
    "                continue\n",
    "                \n",
    "            volume[keys_to_add[i]] = values_to_add[i]\n",
    "            \n",
    "        volume[\"start_date\"] = start_date\n",
    "        volume[\"end_date\"] = end_date\n",
    "        \n",
    "        for key in keys_to_delete:            \n",
    "            del volume[key]\n",
    "                    \n",
    "        with open(\"temp.json\", 'w', encoding=\"utf-8\") as outfile:\n",
    "            json.dump(volume, outfile)               \n",
    "        s3_client.upload_file(\"temp.json\", target_bucket, str(volume[\"identifier\"]) + \".json\", ExtraArgs={'ContentType': \"application/json\"})\n",
    "        volumes += 1\n",
    "        \n",
    "    return \"Metadata for \" + str(volumes) + \" volumes uploaded to S3.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sensitive-belle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Metadata for 30 volumes uploaded to S3.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "build_volume_records(\"us-vol.json\", \"ssda-volume-metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "textile-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def build_cloudsearch_batch(json_path, output_dir):\n",
    "    \n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_client = boto3.client('s3')\n",
    "    #bucket = s3_resource.Bucket(target_bucket)    \n",
    "    \n",
    "    volumes = 0\n",
    "    batch = []\n",
    "    \n",
    "    with open(json_path, encoding=\"utf-8\") as jsonfile:\n",
    "        data = json.load(jsonfile)\n",
    "        \n",
    "    for volume in data[\"volumes\"]:\n",
    "        keys_to_delete = [\"s3_path\", \"contributor\"]\n",
    "        keys_to_add = []\n",
    "        values_to_add = []\n",
    "        for key in volume:\n",
    "            if key in keys_to_delete:                \n",
    "                continue\n",
    "            elif (type(volume[key]) == list) and ((key == \"creator\") or (key == \"format\")):                \n",
    "                temp = ''\n",
    "                for element in volume[key]:\n",
    "                    temp += element + \"; \"\n",
    "                temp = temp[:len(temp) - 2]\n",
    "                volume[key] = temp\n",
    "            elif key == \"date\":\n",
    "                if volume[key][\"start year\"] == None:\n",
    "                    start_date = \"1500\"\n",
    "                else:\n",
    "                    start_date = str(volume[key][\"start year\"])\n",
    "                if volume[key][\"start month\"] != None:\n",
    "                    str_start_month = str(volume[key][\"start month\"])\n",
    "                    start_date += '-' + '0' * (2 - len(str_start_month)) + str_start_month\n",
    "                    if volume[key][\"start day\"] != None:\n",
    "                        str_start_day = str(volume[key][\"start day\"])\n",
    "                        start_date += '-' + '0' * (2 - len(str_start_day)) + str_start_day\n",
    "                if volume[key][\"end year\"] == None:\n",
    "                    end_date = \"2000\"\n",
    "                else:\n",
    "                    end_date = str(volume[key][\"end year\"])\n",
    "                if volume[key][\"end month\"] != None:\n",
    "                    str_end_month = str(volume[key][\"end month\"])\n",
    "                    end_date += '-' + '0' * (2 - len(str_end_month)) + str_end_month\n",
    "                    if volume[key][\"end day\"] != None:\n",
    "                        str_end_day = str(volume[key][\"end day\"])\n",
    "                        end_date += '-' + '0' * (2 - len(str_end_day)) + str_end_day                \n",
    "                keys_to_delete.append(key)                \n",
    "            elif type(volume[key]) == dict:\n",
    "                for key_key in volume[key]:\n",
    "                    keys_to_add.append(key_key)\n",
    "                    values_to_add.append(volume[key][key_key])\n",
    "                keys_to_delete.append(key)\n",
    "            elif volume[key] == None:\n",
    "                volume[key] = ''\n",
    "                \n",
    "        for i in range(len(keys_to_add)):\n",
    "            if values_to_add[i] == None:\n",
    "                values_to_add[i] = ''\n",
    "            \n",
    "            if (keys_to_add[i] == \"coords\") and (values_to_add[i] != ''):\n",
    "                coords = values_to_add[i].split(',')\n",
    "                if ';' in coords[1]:\n",
    "                    coords[1] = coords[1][:coords[1].find(';')]\n",
    "                volume[keys_to_add[i]] = coords[0] + \", \" + coords[1]\n",
    "                continue\n",
    "            elif keys_to_add[i] == \"coords\":\n",
    "                volume[keys_to_add[i]] = \"0, 0\"\n",
    "                continue\n",
    "                \n",
    "            volume[keys_to_add[i]] = values_to_add[i]            \n",
    "        \n",
    "        if len(start_date) == 9:\n",
    "            date_parts = start_date.split('-')\n",
    "            if len(date_parts) == 2:\n",
    "                start_date = start_date[:4]\n",
    "            else:           \n",
    "                date_parts = start_date.split('-')                \n",
    "                if len(date_parts[0]) == 1:                    \n",
    "                    start_date = date_parts[2] + '-0' + date_parts[0] + '-' + date_parts[1]\n",
    "                else:\n",
    "                    start_date = date_parts[2] + '-' + date_parts[1] + '-0' + date_parts[0]\n",
    "            \n",
    "        if len(end_date) == 9:\n",
    "            date_parts = end_date.split('-')\n",
    "            if len(date_parts) == 2:                \n",
    "                end_date = end_date[5:]\n",
    "            else:                \n",
    "                date_parts = end_date.split('-')\n",
    "                if len(date_parts[0]) == 1:\n",
    "                    end_date = date_parts[2] + '-0' + date_parts[0] + '-' + date_parts[1]\n",
    "                else:\n",
    "                    end_date = date_parts[2] + '-' + date_parts[1] + '-0' + date_parts[0]\n",
    "                    \n",
    "        long_months = [\"01\", \"03\", \"05\", \"07\", \"08\", \"10\", \"12\"]\n",
    "        date_parts = end_date.split('-')\n",
    "        \n",
    "        if not ('1' in start_date):\n",
    "            start_date = \"Unknown\"\n",
    "        if not ('1' in end_date):\n",
    "            end_date = \"Unknown\"\n",
    "        if \"??\" in start_date:\n",
    "            start_date = start_date.replace(\"??\", \"01\")\n",
    "        if end_date[5:7] == \"??\":\n",
    "            end_date = end_date[:5] + \"12\" + end_date[7:]\n",
    "        if end_date[8:] == \"??\":\n",
    "            if date_parts[1] in long_months:\n",
    "                end_date = end_date[:8] + \"31\"\n",
    "            elif date_parts[1] == \"02\":\n",
    "                end_date = end_date[:8] + \"28\"\n",
    "            else:\n",
    "                end_date = end_date[:8] + \"30\"\n",
    "                \n",
    "        start_date = start_date.replace('+', '-')\n",
    "        end_date = end_date.replace('+', '-')\n",
    "        start_date = start_date.replace('00', '01')\n",
    "        end_date = end_date.replace('00', '01')\n",
    "        \n",
    "        if ',' in start_date:\n",
    "            start_date = start_date[:4]\n",
    "        if ',' in end_date:\n",
    "            end_date = end_date.replace(' ', '')\n",
    "            end_date = end_date[-4:]            \n",
    "        \n",
    "        date_parts = end_date.split('-')\n",
    "        \n",
    "        if len(start_date) == 10:\n",
    "            volume[\"start_date\"] = start_date + \"T00:00:00Z\"\n",
    "        elif start_date == \"Unknown\":\n",
    "            volume[\"start_date\"] = \"1500-01-01T00:00:00Z\"\n",
    "        elif len(start_date) == 7:\n",
    "            volume[\"start_date\"] = start_date + \"-01T00:00:00Z\"\n",
    "        else:\n",
    "            volume[\"start_date\"] = start_date + \"-01-01T00:00:00Z\"       \n",
    "        \n",
    "        if len(date_parts) == 2:\n",
    "            if date_parts[1] in long_months:\n",
    "                end_month_length = \"31\"\n",
    "            elif date_parts[1] == \"02\":\n",
    "                end_month_length = \"28\"\n",
    "            else:\n",
    "                end_month_length = \"30\"            \n",
    "        \n",
    "        if len(end_date) == 10:\n",
    "            volume[\"end_date\"] = end_date + \"T23:59:59Z\"\n",
    "        elif end_date == \"Unknown\":\n",
    "            volume[\"end_date\"] = \"1999-12-31T23:59:59Z\"\n",
    "        elif len(end_date) == 7:\n",
    "            volume[\"end_date\"] = end_date + '-' + end_month_length + \"T23:59:59Z\"\n",
    "        else:\n",
    "            volume[\"end_date\"] = end_date + \"-12-31T23:59:59Z\"\n",
    "        \n",
    "        for key in keys_to_delete:            \n",
    "            del volume[key]\n",
    "                    \n",
    "        batch_record = {\"type\": \"add\", \"id\": str(volume[\"identifier\"]), \"fields\": volume}\n",
    "        batch.append(batch_record)\n",
    "        volumes += 1\n",
    "        \n",
    "    with open(output_dir, 'w', encoding=\"utf-8\") as outfile:\n",
    "        json.dump(batch, outfile)   \n",
    "        \n",
    "    return \"Metadata for \" + str(volumes) + \" volumes combined in a CloudSearch batch request available at \" + output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "comparative-comparative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Metadata for 556 volumes combined in a CloudSearch batch request available at colombia-cs-batch.json'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "build_cloudsearch_batch(\"colombia-vol.json\", \"colombia-cs-batch.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def scrape_bucket(bucket_name, prefix=None):\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_client = boto3.client('s3')\n",
    "    bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    volume_ids = []\n",
    "    titles = []\n",
    "    volume_roots = []\n",
    "    image_counts = []\n",
    "    has_jpg = []\n",
    "    has_tif = []\n",
    "    has_other = []\n",
    "    other = []\n",
    "    has_pdf = []\n",
    "    has_metadata = []\n",
    "    volume_metadata = []\n",
    "    \n",
    "    folders = [\"jpg\", \"tif\", \"metadata\"]   \n",
    "    \n",
    "    for obj in bucket.objects.filter(Prefix = prefix):    \n",
    "        if (len(obj.key.split('/')) >= 4) and (obj.key.split('/')[3].isdigit()) and (obj.key.split('/')[3] != '') and (obj.key.split('/')[3] not in volume_ids):\n",
    "            volume_ids.append(obj.key.split('/')[3])\n",
    "            volume_root = obj.key[:obj.key.find('/', obj.key.find(obj.key.split('/')[3]))]\n",
    "            volume_roots.append(volume_root)            \n",
    "            has_metadata.append(False)\n",
    "            has_pdf.append(False)\n",
    "            has_jpg.append(False)\n",
    "            has_tif.append(False)\n",
    "            has_other.append(False)\n",
    "            other.append(None)\n",
    "            for volume_obj in bucket.objects.filter(Prefix = volume_root):\n",
    "                if \"DC.xml\" in volume_obj.key:                    \n",
    "                    has_metadata[-1] = True\n",
    "                    s3_client.download_file(bucket.name, volume_obj.key, \"temp.xml\")\n",
    "                    vol_dict = ssda_volume_xml_to_dict(\"temp.xml\", volume_root)\n",
    "                    volume_metadata.append(vol_dict)\n",
    "                    if \"title\" in vol_dict:\n",
    "                        titles.append(vol_dict[\"title\"])\n",
    "                    else:\n",
    "                        titles.append(\"no title\")\n",
    "                    os.remove(\"temp.xml\")\n",
    "                elif \"pdf\" in volume_obj.key.lower():\n",
    "                    has_pdf[-1] = True\n",
    "                elif (has_jpg[-1] == False) and (volume_obj.key.lower().split('/')[4] == \"jpg\"):\n",
    "                    has_jpg[-1] = True\n",
    "                elif (has_tif[-1] == False) and (volume_obj.key.lower().split('/')[4] == \"tif\"):\n",
    "                    has_tif[-1] = True\n",
    "                elif (len(volume_obj.key.split('/')) > 5) and (volume_obj.key.lower().split('/')[4] not in folders) and ((other[-1] == None) or (volume_obj.key.lower().split('/')[4] not in other[-1])):\n",
    "                    has_other[-1] = True\n",
    "                    if other[-1] == None:\n",
    "                        other[-1] = volume_obj.key.lower().split('/')[4]\n",
    "                    else:\n",
    "                        other[-1] = other[-1] + '|' + volume_obj.key.lower().split('/')[4]\n",
    "                \n",
    "            image_metadata = []\n",
    "            prod_imgs = None\n",
    "            if has_jpg[-1]:\n",
    "                prod_imgs = \"jpg\"\n",
    "            elif has_tif[-1]:\n",
    "                prod_imgs = \"tif\"\n",
    "                \n",
    "            if prod_imgs == None:\n",
    "                volume_metadata[-1][\"images\"] = []\n",
    "                image_counts.append(0)\n",
    "            else:\n",
    "                bad_images = 0\n",
    "                for image_obj in bucket.objects.filter(Prefix = volume_root + '/' + prod_imgs.upper()):\n",
    "                    if ('.' + prod_imgs) in image_obj.key.lower():\n",
    "                        file_name = image_obj.key[image_obj.key.rfind('/') + 1:image_obj.key.rfind('.')]\n",
    "                        if not file_name.isdigit():\n",
    "                            print(\"found bad image file name at \" + image_obj.key)\n",
    "                            bad_images += 1\n",
    "                            continue\n",
    "                        extension = image_obj.key[image_obj.key.rfind('.') + 1:]\n",
    "                        temp_path = file_name + '.' + extension\n",
    "                        s3_client.download_file(bucket.name, image_obj.key, temp_path)\n",
    "                        im = Image.open(temp_path)\n",
    "                        width, height = im.size\n",
    "                        im.close()\n",
    "                        os.remove(temp_path)\n",
    "                        image = {\"file_name\": int(file_name), \"extension\": extension, \"height\": height, \"width\": width}\n",
    "                        image_metadata.append(image)\n",
    "                volume_metadata[-1][\"images\"] = image_metadata\n",
    "                image_counts.append(len(image_metadata) + bad_images)           \n",
    "            \n",
    "            print(\"Completed \" + titles[-1])\n",
    "        elif (len(obj.key.split('/')) >= 5) and (not obj.key.split('/')[3].isdigit()) and (obj.key.split('/')[4] != '') and (obj.key.split('/')[4] not in volume_ids):\n",
    "            volume_ids.append(obj.key.split('/')[4])\n",
    "            volume_root = obj.key[:obj.key.find('/', obj.key.find(obj.key.split('/')[4]))]\n",
    "            volume_roots.append(volume_root)            \n",
    "            has_metadata.append(False)\n",
    "            has_pdf.append(False)\n",
    "            has_jpg.append(False)\n",
    "            has_tif.append(False)\n",
    "            has_other.append(False)\n",
    "            other.append(None)\n",
    "            for volume_obj in bucket.objects.filter(Prefix = volume_root):\n",
    "                if \"DC.xml\" in volume_obj.key:                    \n",
    "                    has_metadata[-1] = True\n",
    "                    s3_client.download_file(bucket.name, volume_obj.key, \"temp.xml\")\n",
    "                    vol_dict = ssda_volume_xml_to_dict(\"temp.xml\", volume_root)\n",
    "                    volume_metadata.append(vol_dict)\n",
    "                    if (\"title\" in vol_dict) and (vol_dict[\"title\"] != None):\n",
    "                        titles.append(vol_dict[\"title\"])\n",
    "                    else:\n",
    "                        titles.append(\"no title\")\n",
    "                    os.remove(\"temp.xml\")\n",
    "                elif \"pdf\" in volume_obj.key.lower():\n",
    "                    has_pdf[-1] = True\n",
    "                elif (has_jpg[-1] == False) and (volume_obj.key.lower().split('/')[5] == \"jpg\"):\n",
    "                    has_jpg[-1] = True\n",
    "                elif (has_tif[-1] == False) and (volume_obj.key.lower().split('/')[5] == \"tif\"):\n",
    "                    has_tif[-1] = True\n",
    "                elif (len(volume_obj.key.split('/')) > 6) and (volume_obj.key.lower().split('/')[5] not in folders) and ((other[-1] == None) or (volume_obj.key.lower().split('/')[5] not in other[-1])):\n",
    "                    has_other[-1] = True\n",
    "                    if other[-1] == None:\n",
    "                        other[-1] = volume_obj.key.lower().split('/')[5]\n",
    "                    else:\n",
    "                        other[-1] = other[-1] + '|' + volume_obj.key.lower().split('/')[5]\n",
    "                        \n",
    "            if has_metadata[-1] == False:\n",
    "                titles.append(\"no title\")\n",
    "                print(\"Failed to find metadata for \" + volume_root)\n",
    "                \n",
    "            image_metadata = []\n",
    "            prod_imgs = None\n",
    "            if has_jpg[-1]:\n",
    "                prod_imgs = \"jpg\"\n",
    "            elif has_tif[-1]:\n",
    "                prod_imgs = \"tif\"\n",
    "                \n",
    "            if prod_imgs == None:\n",
    "                volume_metadata[-1][\"images\"] = []\n",
    "                image_counts.append(0)\n",
    "            else:\n",
    "                bad_images = 0\n",
    "                for image_obj in bucket.objects.filter(Prefix = volume_root + '/' + prod_imgs.upper()):\n",
    "                    if ('.' + prod_imgs) in image_obj.key.lower():\n",
    "                        file_name = image_obj.key[image_obj.key.rfind('/') + 1:image_obj.key.rfind('.')]\n",
    "                        if not file_name.isdigit():\n",
    "                            print(\"found incorrect image file name at \" + image_obj.key)\n",
    "                            bad_images += 1\n",
    "                            continue\n",
    "                        extension = image_obj.key[image_obj.key.rfind('.') + 1:]\n",
    "                        temp_path = file_name + '.' + extension\n",
    "                        s3_client.download_file(bucket.name, image_obj.key, temp_path)\n",
    "                        try:\n",
    "                            im = Image.open(temp_path)\n",
    "                            width, height = im.size\n",
    "                            im.close()                        \n",
    "                            image = {\"file_name\": int(file_name), \"extension\": extension, \"height\": height, \"width\": width}\n",
    "                            image_metadata.append(image)\n",
    "                        except:\n",
    "                            print(\"found bad image file at \" + image_obj.key)\n",
    "                            bad_images += 1\n",
    "                        os.remove(temp_path)\n",
    "                volume_metadata[-1][\"images\"] = image_metadata\n",
    "                image_counts.append(len(image_metadata) + bad_images)           \n",
    "            \n",
    "            try:\n",
    "                print(\"Completed \" + titles[-1])\n",
    "            except:\n",
    "                print(\"Completed\")\n",
    "                print(titles[-1])\n",
    "                \n",
    "    volumes_dict = {\"id\": volume_ids, \"title\": titles, \"images\": image_counts, \"s3 root\": volume_roots, \"metadata\": has_metadata, \"has pdf\": has_pdf, \"has jpg\": has_jpg, \"has tif\": has_tif, \"has other\": has_other, \"other\": other}\n",
    "    volumes_df = pd.DataFrame.from_dict(volumes_dict)\n",
    "    \n",
    "    return volumes_df, volume_metadata  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def ssda_volume_xml_to_dict(volume_xml, s3_path):\n",
    "    import xml.etree.ElementTree as ET\n",
    "    tree = ET.parse(volume_xml)\n",
    "    root = tree.getroot()\n",
    "    volume_dict = {}\n",
    "    volume_dict[\"s3_path\"] = s3_path\n",
    "    for item in root:\n",
    "        if \"{http://purl.org/dc/elements/1.1/}\" in item.tag:\n",
    "            item.tag = item.tag[item.tag.find('}') + 1:]\n",
    "        if item.text == None:\n",
    "            if item.tag not in volume_dict:\n",
    "                volume_dict[item.tag] = None\n",
    "            continue\n",
    "        if item.text[0] == ' ':\n",
    "            item.text = item.text[1:]        \n",
    "        if item.tag == \"subject\":\n",
    "            if \"subject\" in volume_dict:\n",
    "                volume_dict[\"subject\"].append(item.text.split(\"--\"))\n",
    "            else:\n",
    "                volume_dict[\"subject\"] = item.text.split(\"--\")\n",
    "        elif item.tag == \"title\":\n",
    "            volume_dict[\"title\"] = item.text\n",
    "        elif item.tag == \"contributor\":\n",
    "            if (item.text.find('(') != -1) and (item.text.find(')') != -1):\n",
    "                name = item.text[:item.text.find('(')]\n",
    "                role = item.text[item.text.find('(') + 1:item.text.find(')')]\n",
    "            else:\n",
    "                continue\n",
    "            if \"contributor\" in volume_dict:\n",
    "                volume_dict[\"contributor\"].append({\"name\": name, \"role\": role})\n",
    "            else:\n",
    "                volume_dict[\"contributor\"] = [{\"name\": name, \"role\": role}]\n",
    "        elif item.tag == \"identifier\":\n",
    "            volume_dict[\"identifier\"] = item.text[item.text.find(':') + 1:]\n",
    "        elif item.tag == \"coverage\":\n",
    "            if ('.' in item.text) and (',' in item.text) and (\"Archives\" not in item.text):\n",
    "                if \"coverage\" in volume_dict:\n",
    "                    volume_dict[\"coverage\"][\"coords\"] = item.text\n",
    "                else:\n",
    "                    volume_dict[\"coverage\"] = {\"coords\": item.text}\n",
    "            elif \"--\" in item.text:\n",
    "                places = item.text.split(\"--\")\n",
    "                if len(places) == 4:\n",
    "                    if \"coverage\" in volume_dict:\n",
    "                        volume_dict[\"coverage\"][\"country\"] = places[1]\n",
    "                        volume_dict[\"coverage\"][\"state\"] = places[2]\n",
    "                        volume_dict[\"coverage\"][\"city\"] = places[3]\n",
    "                    else:\n",
    "                        volume_dict[\"coverage\"] = {\"country\": places[1], \"state\": places[2], \"city\": places[3]}                \n",
    "        elif item.tag == \"source\":\n",
    "            if \"coverage\" in volume_dict:\n",
    "                volume_dict[\"coverage\"][\"institution\"] = item.text\n",
    "            else:\n",
    "                volume_dict[\"coverage\"] = {\"institution\": item.text}\n",
    "        elif ((item.tag == \"type\") and (item.text == \"Text\")) or (item.tag == \"rights\"):\n",
    "            continue\n",
    "        elif (item.tag == \"creator\") and (';' in item.text):            \n",
    "            creators = item.text.split(';')\n",
    "            for creator in creators:                \n",
    "                if (len(creator) > 1) and (creator[0] == ' '):\n",
    "                    creator = creator[1:]\n",
    "                if \"creator\" in volume_dict:\n",
    "                    volume_dict[\"creator\"].append(creator)\n",
    "                else:\n",
    "                    volume_dict[\"creator\"] = [creator]\n",
    "        elif (item.tag == \"language\") and (';' in item.text):\n",
    "            languages = item.text.split(';')\n",
    "            for language in languages:\n",
    "                if (len(language) > 1) and (language[0] == ' '):\n",
    "                    language = language[1:]\n",
    "                if \"language\" in volume_dict:\n",
    "                    volume_dict[\"language\"].append(language)\n",
    "                else:\n",
    "                    volume_dict[\"language\"] = [language]\n",
    "        else:            \n",
    "            if item.tag in volume_dict:                \n",
    "                volume_dict[item.tag].append(item.text)                \n",
    "            else:\n",
    "                volume_dict[item.tag] = [item.text]       \n",
    "        \n",
    "    if \"coverage\" not in volume_dict:\n",
    "        volume_dict[\"coverage\"] = {}               \n",
    "        volume_dict[\"coverage\"][\"country\"] = volume_dict[\"s3_path\"].split('/')[0].replace('_', ' ')\n",
    "        volume_dict[\"coverage\"][\"state\"] = volume_dict[\"s3_path\"].split('/')[1].replace('_', ' ')\n",
    "        volume_dict[\"coverage\"][\"city\"] = volume_dict[\"s3_path\"].split('/')[2].replace('_', ' ')\n",
    "        volume_dict[\"coverage\"][\"institution\"] = volume_dict[\"s3_path\"].split('/')[3].replace('_', ' ')\n",
    "    elif \"institution\" not in volume_dict[\"coverage\"]:\n",
    "        volume_dict[\"coverage\"][\"institution\"] = volume_dict[\"s3_path\"].split('/')[3].replace('_', ' ')\n",
    "        \n",
    "    return volume_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-peripheral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Libro de Venta de Esclavo\n",
      "Finished working on Colombia_Choc_Quibd_Notaria_Primera_de_Quibd_5697\n"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "prefixes = [\"Colombia/Choc/Quibd/Notaria_Primera_de_Quibd/56975\"]\n",
    "\n",
    "for pref in prefixes:\n",
    "    df, metadata = scrape_bucket(\"ssda-assets\", prefix=pref)\n",
    "    pref = pref.replace('/', '_')\n",
    "    pref = pref[:-1]\n",
    "    df.to_csv(pref.lower() + \".csv\", index = False)\n",
    "    with open(pref.lower() + \".json\", 'w', encoding=\"utf-8\") as outfile:\n",
    "        outfile.write('{\\n\\\"volumes\\\": \\n')\n",
    "        json.dump(metadata, outfile)\n",
    "        outfile.write('}')\n",
    "    print(\"Finished working on \" + pref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-concert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted manifest_generation.ipynb.\n",
      "Converted s3_scrape.ipynb.\n",
      "Converted s3_scrape_dev.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-consensus",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
