{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp s3_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def scrape_bucket(bucket_name, prefix=None):\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_client = boto3.client('s3')\n",
    "    bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    volume_ids = []\n",
    "    titles = []\n",
    "    volume_roots = []\n",
    "    image_counts = []\n",
    "    has_jpg = []\n",
    "    has_tif = []\n",
    "    has_other = []\n",
    "    other = []\n",
    "    has_pdf = []\n",
    "    has_metadata = []\n",
    "    volume_metadata = []\n",
    "    \n",
    "    folders = [\"jpg\", \"tif\", \"metadata\"]   \n",
    "    \n",
    "    for obj in bucket.objects.filter(Prefix = prefix):    \n",
    "        if (len(obj.key.split('/')) >= 4) and (obj.key.split('/')[3].isdigit()) and (obj.key.split('/')[3] != '') and (obj.key.split('/')[3] not in volume_ids):\n",
    "            volume_ids.append(obj.key.split('/')[3])\n",
    "            volume_root = obj.key[:obj.key.find('/', obj.key.find(obj.key.split('/')[3]))]\n",
    "            volume_roots.append(volume_root)            \n",
    "            has_metadata.append(False)\n",
    "            has_pdf.append(False)\n",
    "            has_jpg.append(False)\n",
    "            has_tif.append(False)\n",
    "            has_other.append(False)\n",
    "            other.append(None)\n",
    "            for volume_obj in bucket.objects.filter(Prefix = volume_root):\n",
    "                if \"DC.xml\" in volume_obj.key:                    \n",
    "                    has_metadata[-1] = True\n",
    "                    s3_client.download_file(bucket.name, volume_obj.key, \"temp.xml\")\n",
    "                    vol_dict = ssda_volume_xml_to_dict(\"temp.xml\", volume_root)\n",
    "                    volume_metadata.append(vol_dict)\n",
    "                    if \"title\" in vol_dict:\n",
    "                        titles.append(vol_dict[\"title\"])\n",
    "                    else:\n",
    "                        titles.append(\"no title\")\n",
    "                    os.remove(\"temp.xml\")\n",
    "                elif \"pdf\" in volume_obj.key.lower():\n",
    "                    has_pdf[-1] = True\n",
    "                elif (has_jpg[-1] == False) and (volume_obj.key.lower().split('/')[4] == \"jpg\"):\n",
    "                    has_jpg[-1] = True\n",
    "                elif (has_tif[-1] == False) and (volume_obj.key.lower().split('/')[4] == \"tif\"):\n",
    "                    has_tif[-1] = True\n",
    "                elif (len(volume_obj.key.split('/')) > 5) and (volume_obj.key.lower().split('/')[4] not in folders) and ((other[-1] == None) or (volume_obj.key.lower().split('/')[4] not in other[-1])):\n",
    "                    has_other[-1] = True\n",
    "                    if other[-1] == None:\n",
    "                        other[-1] = volume_obj.key.lower().split('/')[4]\n",
    "                    else:\n",
    "                        other[-1] = other[-1] + '|' + volume_obj.key.lower().split('/')[4]\n",
    "                \n",
    "            image_metadata = []\n",
    "            prod_imgs = None\n",
    "            if has_jpg[-1]:\n",
    "                prod_imgs = \"jpg\"\n",
    "            elif has_tif[-1]:\n",
    "                prod_imgs = \"tif\"\n",
    "                \n",
    "            if prod_imgs == None:\n",
    "                volume_metadata[-1][\"images\"] = []\n",
    "                image_counts.append(0)\n",
    "            else:\n",
    "                bad_images = 0\n",
    "                for image_obj in bucket.objects.filter(Prefix = volume_root + '/' + prod_imgs.upper()):\n",
    "                    if ('.' + prod_imgs) in image_obj.key.lower():\n",
    "                        file_name = image_obj.key[image_obj.key.rfind('/') + 1:image_obj.key.rfind('.')]\n",
    "                        if not file_name.isdigit():\n",
    "                            print(\"found bad image file name at \" + image_obj.key)\n",
    "                            bad_images += 1\n",
    "                            continue\n",
    "                        extension = image_obj.key[image_obj.key.rfind('.') + 1:]\n",
    "                        temp_path = file_name + '.' + extension\n",
    "                        s3_client.download_file(bucket.name, image_obj.key, temp_path)\n",
    "                        im = Image.open(temp_path)\n",
    "                        width, height = im.size\n",
    "                        im.close()\n",
    "                        os.remove(temp_path)\n",
    "                        image = {\"file_name\": int(file_name), \"extension\": extension, \"height\": height, \"width\": width}\n",
    "                        image_metadata.append(image)\n",
    "                volume_metadata[-1][\"images\"] = image_metadata\n",
    "                image_counts.append(len(image_metadata) + bad_images)           \n",
    "            \n",
    "            print(\"Completed \" + titles[-1])\n",
    "        elif (len(obj.key.split('/')) >= 5) and (not obj.key.split('/')[3].isdigit()) and (obj.key.split('/')[4] != '') and (obj.key.split('/')[4] not in volume_ids):\n",
    "            volume_ids.append(obj.key.split('/')[4])\n",
    "            volume_root = obj.key[:obj.key.find('/', obj.key.find(obj.key.split('/')[4]))]\n",
    "            volume_roots.append(volume_root)            \n",
    "            has_metadata.append(False)\n",
    "            has_pdf.append(False)\n",
    "            has_jpg.append(False)\n",
    "            has_tif.append(False)\n",
    "            has_other.append(False)\n",
    "            other.append(None)\n",
    "            for volume_obj in bucket.objects.filter(Prefix = volume_root):\n",
    "                if \"DC.xml\" in volume_obj.key:                    \n",
    "                    has_metadata[-1] = True\n",
    "                    s3_client.download_file(bucket.name, volume_obj.key, \"temp.xml\")\n",
    "                    vol_dict = ssda_volume_xml_to_dict(\"temp.xml\", volume_root)\n",
    "                    volume_metadata.append(vol_dict)\n",
    "                    if (\"title\" in vol_dict) and (vol_dict[\"title\"] != None):\n",
    "                        titles.append(vol_dict[\"title\"])\n",
    "                    else:\n",
    "                        titles.append(\"no title\")\n",
    "                    os.remove(\"temp.xml\")\n",
    "                elif \"pdf\" in volume_obj.key.lower():\n",
    "                    has_pdf[-1] = True\n",
    "                elif (has_jpg[-1] == False) and (volume_obj.key.lower().split('/')[5] == \"jpg\"):\n",
    "                    has_jpg[-1] = True\n",
    "                elif (has_tif[-1] == False) and (volume_obj.key.lower().split('/')[5] == \"tif\"):\n",
    "                    has_tif[-1] = True\n",
    "                elif (len(volume_obj.key.split('/')) > 6) and (volume_obj.key.lower().split('/')[5] not in folders) and ((other[-1] == None) or (volume_obj.key.lower().split('/')[5] not in other[-1])):\n",
    "                    has_other[-1] = True\n",
    "                    if other[-1] == None:\n",
    "                        other[-1] = volume_obj.key.lower().split('/')[5]\n",
    "                    else:\n",
    "                        other[-1] = other[-1] + '|' + volume_obj.key.lower().split('/')[5]\n",
    "                        \n",
    "            if has_metadata[-1] == False:\n",
    "                titles.append(\"no title\")\n",
    "                print(\"Failed to find metadata for \" + volume_root)\n",
    "                \n",
    "            image_metadata = []\n",
    "            prod_imgs = None\n",
    "            if has_jpg[-1]:\n",
    "                prod_imgs = \"jpg\"\n",
    "            elif has_tif[-1]:\n",
    "                prod_imgs = \"tif\"\n",
    "                \n",
    "            if prod_imgs == None:\n",
    "                volume_metadata[-1][\"images\"] = []\n",
    "                image_counts.append(0)\n",
    "            else:\n",
    "                bad_images = 0\n",
    "                for image_obj in bucket.objects.filter(Prefix = volume_root + '/' + prod_imgs.upper()):\n",
    "                    if ('.' + prod_imgs) in image_obj.key.lower():\n",
    "                        file_name = image_obj.key[image_obj.key.rfind('/') + 1:image_obj.key.rfind('.')]\n",
    "                        if not file_name.isdigit():\n",
    "                            print(\"found incorrect image file name at \" + image_obj.key)\n",
    "                            bad_images += 1\n",
    "                            continue\n",
    "                        extension = image_obj.key[image_obj.key.rfind('.') + 1:]\n",
    "                        temp_path = file_name + '.' + extension\n",
    "                        s3_client.download_file(bucket.name, image_obj.key, temp_path)\n",
    "                        try:\n",
    "                            im = Image.open(temp_path)\n",
    "                            width, height = im.size\n",
    "                            im.close()                        \n",
    "                            image = {\"file_name\": int(file_name), \"extension\": extension, \"height\": height, \"width\": width}\n",
    "                            image_metadata.append(image)\n",
    "                        except:\n",
    "                            print(\"found bad image file at \" + image_obj.key)\n",
    "                            bad_images += 1\n",
    "                        os.remove(temp_path)\n",
    "                volume_metadata[-1][\"images\"] = image_metadata\n",
    "                image_counts.append(len(image_metadata) + bad_images)           \n",
    "            \n",
    "            try:\n",
    "                print(\"Completed \" + titles[-1])\n",
    "            except:\n",
    "                print(\"Completed\")\n",
    "                print(titles[-1])\n",
    "                \n",
    "    volumes_dict = {\"id\": volume_ids, \"title\": titles, \"images\": image_counts, \"s3 root\": volume_roots, \"metadata\": has_metadata, \"has pdf\": has_pdf, \"has jpg\": has_jpg, \"has tif\": has_tif, \"has other\": has_other, \"other\": other}\n",
    "    volumes_df = pd.DataFrame.from_dict(volumes_dict)\n",
    "    \n",
    "    return volumes_df, volume_metadata  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def ssda_volume_xml_to_dict(volume_xml, s3_path):\n",
    "    import xml.etree.ElementTree as ET\n",
    "    tree = ET.parse(volume_xml)\n",
    "    root = tree.getroot()\n",
    "    volume_dict = {}\n",
    "    volume_dict[\"s3_path\"] = s3_path\n",
    "    for item in root:\n",
    "        if \"{http://purl.org/dc/elements/1.1/}\" in item.tag:\n",
    "            item.tag = item.tag[item.tag.find('}') + 1:]\n",
    "        if item.text == None:\n",
    "            if item.tag not in volume_dict:\n",
    "                volume_dict[item.tag] = None\n",
    "            continue\n",
    "        if item.text[0] == ' ':\n",
    "            item.text = item.text[1:]        \n",
    "        if item.tag == \"subject\":\n",
    "            if \"subject\" in volume_dict:\n",
    "                volume_dict[\"subject\"].append(item.text.split(\"--\"))\n",
    "            else:\n",
    "                volume_dict[\"subject\"] = item.text.split(\"--\")\n",
    "        elif item.tag == \"title\":\n",
    "            volume_dict[\"title\"] = item.text\n",
    "        elif item.tag == \"contributor\":\n",
    "            if (item.text.find('(') != -1) and (item.text.find(')') != -1):\n",
    "                name = item.text[:item.text.find('(')]\n",
    "                role = item.text[item.text.find('(') + 1:item.text.find(')')]\n",
    "            else:\n",
    "                continue\n",
    "            if \"contributor\" in volume_dict:\n",
    "                volume_dict[\"contributor\"].append({\"name\": name, \"role\": role})\n",
    "            else:\n",
    "                volume_dict[\"contributor\"] = [{\"name\": name, \"role\": role}]\n",
    "        elif item.tag == \"identifier\":\n",
    "            volume_dict[\"identifier\"] = item.text[item.text.find(':') + 1:]\n",
    "        elif item.tag == \"coverage\":\n",
    "            if ('.' in item.text) and (',' in item.text) and (\"Archives\" not in item.text):\n",
    "                if \"coverage\" in volume_dict:\n",
    "                    volume_dict[\"coverage\"][\"coords\"] = item.text\n",
    "                else:\n",
    "                    volume_dict[\"coverage\"] = {\"coords\": item.text}\n",
    "            elif \"--\" in item.text:\n",
    "                places = item.text.split(\"--\")\n",
    "                if len(places) == 4:\n",
    "                    if \"coverage\" in volume_dict:\n",
    "                        volume_dict[\"coverage\"][\"country\"] = places[1]\n",
    "                        volume_dict[\"coverage\"][\"state\"] = places[2]\n",
    "                        volume_dict[\"coverage\"][\"city\"] = places[3]\n",
    "                    else:\n",
    "                        volume_dict[\"coverage\"] = {\"country\": places[1], \"state\": places[2], \"city\": places[3]}                \n",
    "        elif item.tag == \"source\":\n",
    "            if \"coverage\" in volume_dict:\n",
    "                volume_dict[\"coverage\"][\"institution\"] = item.text\n",
    "            else:\n",
    "                volume_dict[\"coverage\"] = {\"institution\": item.text}\n",
    "        elif ((item.tag == \"type\") and (item.text == \"Text\")) or (item.tag == \"rights\"):\n",
    "            continue\n",
    "        elif (item.tag == \"creator\") and (';' in item.text):            \n",
    "            creators = item.text.split(';')\n",
    "            for creator in creators:                \n",
    "                if (len(creator) > 1) and (creator[0] == ' '):\n",
    "                    creator = creator[1:]\n",
    "                if \"creator\" in volume_dict:\n",
    "                    volume_dict[\"creator\"].append(creator)\n",
    "                else:\n",
    "                    volume_dict[\"creator\"] = [creator]\n",
    "        elif (item.tag == \"language\") and (';' in item.text):\n",
    "            languages = item.text.split(';')\n",
    "            for language in languages:\n",
    "                if (len(language) > 1) and (language[0] == ' '):\n",
    "                    language = language[1:]\n",
    "                if \"language\" in volume_dict:\n",
    "                    volume_dict[\"language\"].append(language)\n",
    "                else:\n",
    "                    volume_dict[\"language\"] = [language]\n",
    "        else:            \n",
    "            if item.tag in volume_dict:                \n",
    "                volume_dict[item.tag].append(item.text)                \n",
    "            else:\n",
    "                volume_dict[item.tag] = [item.text]       \n",
    "        \n",
    "    if \"coverage\" not in volume_dict:\n",
    "        volume_dict[\"coverage\"] = {}               \n",
    "        volume_dict[\"coverage\"][\"country\"] = volume_dict[\"s3_path\"].split('/')[0].replace('_', ' ')\n",
    "        volume_dict[\"coverage\"][\"state\"] = volume_dict[\"s3_path\"].split('/')[1].replace('_', ' ')\n",
    "        volume_dict[\"coverage\"][\"city\"] = volume_dict[\"s3_path\"].split('/')[2].replace('_', ' ')\n",
    "        volume_dict[\"coverage\"][\"institution\"] = volume_dict[\"s3_path\"].split('/')[3].replace('_', ' ')\n",
    "    elif \"institution\" not in volume_dict[\"coverage\"]:\n",
    "        volume_dict[\"coverage\"][\"institution\"] = volume_dict[\"s3_path\"].split('/')[3].replace('_', ' ')\n",
    "        \n",
    "    return volume_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-peripheral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Libro de Venta de Esclavo\n",
      "Finished working on Colombia_Chocó_Quibdó_Notaria_Primera_de_Quibdó_5697\n"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "prefixes = [\"Colombia/Chocó/Quibdó/Notaria_Primera_de_Quibdó/56975\"]\n",
    "\n",
    "for pref in prefixes:\n",
    "    df, metadata = scrape_bucket(\"ssda-assets\", prefix=pref)\n",
    "    pref = pref.replace('/', '_')\n",
    "    pref = pref[:-1]\n",
    "    df.to_csv(pref.lower() + \".csv\", index = False)\n",
    "    with open(pref.lower() + \".json\", 'w', encoding=\"utf-8\") as outfile:\n",
    "        outfile.write('{\\n\\\"volumes\\\": \\n')\n",
    "        json.dump(metadata, outfile)\n",
    "        outfile.write('}')\n",
    "    print(\"Finished working on \" + pref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-concert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted manifest_generation.ipynb.\n",
      "Converted s3_scrape.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-consensus",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
