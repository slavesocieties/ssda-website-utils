{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp s3_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def scrape_bucket(bucket_name):\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_client = boto3.client('s3')\n",
    "    bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    volume_ids = []\n",
    "    titles = []\n",
    "    volume_roots = []\n",
    "    image_counts = []#\n",
    "    has_jpg = []\n",
    "    has_tif = []\n",
    "    has_other = []\n",
    "    other = []\n",
    "    has_pdf = []\n",
    "    has_metadata = []\n",
    "    volume_metadata = []\n",
    "    \n",
    "    folders = [\"jpg\", \"tif\", \"metadata\"]    \n",
    "    \n",
    "    for obj in bucket.objects.all():            \n",
    "        if (len(obj.key.split('/')) >= 5) and (obj.key.split('/')[4] not in volume_ids):            \n",
    "            volume_ids.append(obj.key.split('/')[4])\n",
    "            volume_root = obj.key[:obj.key.rfind('/')]\n",
    "            volume_roots.append(volume_root)\n",
    "            has_metadata.append(False)\n",
    "            has_pdf.append(False)\n",
    "            has_jpg.append(False)\n",
    "            has_tif.append(False)\n",
    "            has_other.append(False)\n",
    "            other.append([])\n",
    "            for volume_obj in bucket.objects.filter(Prefix = volume_root):\n",
    "                if \"DC.xml\" in volume_obj.key:\n",
    "                    has_metadata[-1] = True\n",
    "                    s3_client.download_file(bucket.name, volume_obj.key, \"temp.xml\")\n",
    "                    vol_dict = ssda_volume_xml_to_dict(\"temp.xml\", volume_root)\n",
    "                    volume_metadata.append(vol_dict)\n",
    "                    titles.append(vol_dict[\"title\"])\n",
    "                    os.remove(\"temp.xml\")\n",
    "                elif \"pdf\" in volume_obj.key.lower():\n",
    "                    has_pdf[-1] = True\n",
    "                elif (has_jpg[-1] == False) and (volume_obj.key.lower().split('/')[5] == \"jpg\"):\n",
    "                    has_jpg[-1] = True\n",
    "                elif (has_tif[-1] == False) and (volume_obj.key.lower().split('/')[5] == \"tif\"):\n",
    "                    has_tif[-1] = True\n",
    "                elif (len(volume_obj.key.split('/')) > 6) and (volume_obj.key.lower().split('/')[5] not in folders):\n",
    "                    has_other[-1] = True\n",
    "                    other[-1].append(volume_obj.key.lower().split('/')[5])\n",
    "                \n",
    "            image_metadata = []\n",
    "            prod_imgs = None\n",
    "            if has_jpg[-1]:\n",
    "                prod_imgs = \"jpg\"\n",
    "            elif has_tif[-1]:\n",
    "                prod_imgs = \"tif\"\n",
    "                \n",
    "            if prod_imgs == None:\n",
    "                volume_metadata[-1][\"images\"] = []\n",
    "                image_counts.append(0)\n",
    "            else:                \n",
    "                for image_obj in bucket.objects.filter(Prefix = volume_root + '/' + prod_imgs.upper()):\n",
    "                    if ('.' + prod_imgs) in image_obj.key.lower():\n",
    "                        file_name = image_obj.key[image_obj.key.rfind('/') + 1:image_obj.key.rfind('.')]\n",
    "                        extension = image_obj.key[image_obj.key.rfind('.') + 1:]\n",
    "                        temp_path = file_name + '.' + extension\n",
    "                        s3_client.download_file(bucket.name, image_obj.key, temp_path)\n",
    "                        im = Image.open(temp_path)\n",
    "                        width, height = im.size\n",
    "                        im.close()\n",
    "                        os.remove(temp_path)\n",
    "                        image = {\"file_name\": int(file_name), \"extension\": extension, \"height\": height, \"width\": width}\n",
    "                        image_metadata.append(image)\n",
    "                volume_metadata[-1][\"images\"] = image_metadata\n",
    "                image_counts.append(len(image_metadata))            \n",
    "            \n",
    "            if has_metadata[-1] == False:\n",
    "                titles.append(\"no title\")           \n",
    "                \n",
    "    volumes_dict = {\"id\": volume_ids, \"title\": titles, \"images\": image_counts, \"s3 root\": volume_roots, \"metadata\": has_metadata, \"has pdf\": has_pdf, \"has jpg\": has_jpg, \"has tif\": has_tif, \"has other\": has_other, \"other\": other}\n",
    "    volumes_df = pd.DataFrame.from_dict(volumes_dict)\n",
    "    \n",
    "    return volumes_df, volume_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def ssda_volume_xml_to_dict(volume_xml, s3_path):\n",
    "    import xml.etree.ElementTree as ET\n",
    "    tree = ET.parse(volume_xml)\n",
    "    root = tree.getroot()\n",
    "    volume_dict = {}\n",
    "    volume_dict[\"s3_path\"] = s3_path\n",
    "    for item in root:\n",
    "        if item.text == None:\n",
    "            volume_dict[item.tag] = None\n",
    "        if item.text[0] == ' ':\n",
    "            item.text = item.text[1:]        \n",
    "        if item.tag == \"subject\":\n",
    "            if \"subject\" in volume_dict:\n",
    "                volume_dict[\"subject\"].append(item.text.split(\"--\"))\n",
    "            else:\n",
    "                volume_dict[\"subject\"] = item.text.split(\"--\")\n",
    "        elif item.tag == \"title\":\n",
    "            volume_dict[\"title\"] = item.text\n",
    "        elif item.tag == \"contributor\":\n",
    "            if (item.text.find('(') != -1) and (item.text.find(')') != -1):\n",
    "                name = item.text[:item.text.find('(')]\n",
    "                role = item.text[item.text.find('(') + 1:item.text.find(')')]\n",
    "            else:\n",
    "                continue\n",
    "            if \"contributor\" in volume_dict:\n",
    "                volume_dict[\"contributor\"].append({\"name\": name, \"role\": role})\n",
    "            else:\n",
    "                volume_dict[\"contributor\"] = [{\"name\": name, \"role\": role}]\n",
    "        elif item.tag == \"identifier\":\n",
    "            volume_dict[\"identifier\"] = item.text[item.text.find(':') + 1:]\n",
    "        elif item.tag == \"coverage\":\n",
    "            if ('.' in item.text) and (',' in item.text):\n",
    "                if \"coverage\" in volume_dict:\n",
    "                    volume_dict[\"coverage\"][\"coords\"] = item.text\n",
    "                else:\n",
    "                    volume_dict[\"coverage\"] = {\"coords\": item.text}\n",
    "            elif \"--\" in item.text:\n",
    "                places = item.text.split(\"--\")\n",
    "                if len(places) == 4:\n",
    "                    if \"coverage\" in volume_dict:\n",
    "                        volume_dict[\"coverage\"][\"country\"] = places[1]\n",
    "                        volume_dict[\"coverage\"][\"state\"] = places[2]\n",
    "                        volume_dict[\"coverage\"][\"city\"] = places[3]\n",
    "                    else:\n",
    "                        volume_dict[\"coverage\"] = {\"country\": places[1], \"state\": places[2], \"city\": places[3]}                \n",
    "        elif item.tag == \"source\":\n",
    "            if \"coverage\" in volume_dict:\n",
    "                volume_dict[\"coverage\"][\"institution\"] = item.text\n",
    "            else:\n",
    "                volume_dict[\"coverage\"] = {\"institution\": item.text}\n",
    "        elif ((item.tag == \"type\") and (item.text == \"Text\")) or (item.tag == \"rights\"):\n",
    "            continue\n",
    "        else:\n",
    "            if item.tag in volume_dict:                \n",
    "                volume_dict[item.tag].append(item.text)                \n",
    "            else:\n",
    "                volume_dict[item.tag] = [item.text]       \n",
    "        \n",
    "        if \"coverage\" not in volume_dict:\n",
    "            volume_dict[\"coverage\"] = {}\n",
    "            if \"country\" not in volume_dict[\"coverage\"]:            \n",
    "                volume_dict[\"coverage\"][\"country\"] = volume_dict[\"s3_path\"].split('/')[0].replace('_', ' ')\n",
    "                volume_dict[\"coverage\"][\"state\"] = volume_dict[\"s3_path\"].split('/')[1].replace('_', ' ')\n",
    "                volume_dict[\"coverage\"][\"city\"] = volume_dict[\"s3_path\"].split('/')[2].replace('_', ' ')\n",
    "        \n",
    "    return volume_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_test\n",
    "\n",
    "volume_df, volume_metadata = scrape_bucket(\"ssda-assets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-double",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>images</th>\n",
       "      <th>s3 root</th>\n",
       "      <th>metadata</th>\n",
       "      <th>has pdf</th>\n",
       "      <th>has jpg</th>\n",
       "      <th>has tif</th>\n",
       "      <th>has other</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2223</td>\n",
       "      <td>Supplemento do Livro 8 para os assentos de óbi...</td>\n",
       "      <td>69</td>\n",
       "      <td>Brazil/Minas_Gerais/Nova_Lima/Nossa_Senhora_do...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              title  images  \\\n",
       "0  2223  Supplemento do Livro 8 para os assentos de óbi...      69   \n",
       "\n",
       "                                             s3 root  metadata  has pdf  \\\n",
       "0  Brazil/Minas_Gerais/Nova_Lima/Nossa_Senhora_do...      True     True   \n",
       "\n",
       "   has jpg  has tif  has other other  \n",
       "0     True    False      False    []  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "volume_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-apartment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'s3_path': 'Brazil/Minas_Gerais/Nova_Lima/Nossa_Senhora_do_Pilar/2223',\n",
       "  'title': 'Supplemento do Livro 8 para os assentos de óbitos dos escravos da Companhia do Morro Velho',\n",
       "  'coverage': {'country': 'Brazil',\n",
       "   'state': 'Minas Gerais',\n",
       "   'city': 'Nova Lima',\n",
       "   'institution': 'Nossa Senhora do Pilar de Nova Lima',\n",
       "   'coords': '-19.9854, -43.84711'},\n",
       "  'creator': ['National Endowment for the Humanities Collaborative Research Grant'],\n",
       "  'subject': ['Burials', 'Brazil', 'Modern'],\n",
       "  'description': ['Full title: \"Supplemento do Livro 8 desta Freguesia de Congonhas de Sabará para os assentos de óbitos dos escravos da Companhia do Morro Velho desde o primeiro de Abril do corrente ano de 1871.\" The volume contains burial records of enslaved individuals owned by the St. John d\\'El Rey Mining Company (also known as the Companhia do Morro Velho). Good condition'],\n",
       "  'publisher': ['Slave Societies Digital Archive'],\n",
       "  'contributor': [{'name': 'Jane Landers', 'role': 'project director'},\n",
       "   {'name': 'Kara Schultz', 'role': 'metadata creation'}],\n",
       "  'identifier': '2223',\n",
       "  'date': ['1871-04-01/1885-02-28'],\n",
       "  'type': ['Ecclesiastical records'],\n",
       "  'language': ['por'],\n",
       "  'images': [{'file_name': 1001,\n",
       "    'extension': 'JPG',\n",
       "    'height': 3648,\n",
       "    'width': 2736},\n",
       "   {'file_name': 1002, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1003, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1004, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1005, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1006, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1007, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1008, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1009, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1010, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1011, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1012, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1013, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1014, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1015, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1016, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1017, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1018, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1019, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1020, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1021, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1022, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1023, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1024, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1025, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1026, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1027, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1028, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1029, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1030, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1031, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1032, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1033, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1034, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1035, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1036, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1037, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1038, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1039, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1040, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1041, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1042, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1043, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1044, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1045, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1046, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1047, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1048, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1049, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1050, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1051, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1052, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1053, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1054, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1055, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1056, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1057, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1058, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1059, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1060, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1061, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1062, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1063, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1064, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1065, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1066, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1067, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1068, 'extension': 'JPG', 'height': 3648, 'width': 2736},\n",
       "   {'file_name': 1069, 'extension': 'JPG', 'height': 3648, 'width': 2736}]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "volume_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-miracle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted manifest_generation.ipynb.\n",
      "Converted s3_scrape.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-three",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
