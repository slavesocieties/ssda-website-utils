{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp s3_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "precise-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alien-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def copy_jpgs(json_path, source_bucket, target_bucket):\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    images = 0\n",
    "    \n",
    "    with open(json_path, encoding=\"utf-8\") as jsonfile:\n",
    "        data = json.load(jsonfile)\n",
    "        \n",
    "    for volume in data[\"volumes\"]:\n",
    "        print(\"Now working on \" + volume[\"identifier\"])\n",
    "        for image in volume[\"images\"]:            \n",
    "            #copy_source = {\"Bucket\": source_bucket, \"Key\": volume[\"s3_path\"] + \"/JPG/\" + str(image[\"file_name\"]) + \".JPG\"}\n",
    "            try:\n",
    "                s3_client.download_file(source_bucket, volume[\"s3_path\"] + \"/JPG/\" + str(image[\"file_name\"]) + \".JPG\", \"temp.jpg\")\n",
    "            except:\n",
    "                s3_client.download_file(source_bucket, volume[\"s3_path\"] + \"/JPG/\" + str(image[\"file_name\"]) + \".jpg\", \"temp.jpg\")            \n",
    "            image_number = str(image[\"file_name\"] - 1000)\n",
    "            padded_number = '0' * (4 - len(image_number)) + image_number\n",
    "            #s3_client.copy(copy_source, target_bucket, str(volume[\"identifier\"]) + '-' + padded_number + \".jpg\", ExtraArgs={'ContentType': \"image/jpeg\", 'Metadata': {\"x-amz-meta-width\": str(image[\"width\"]), \"x-amz-meta-height\": str(image[\"height\"])}})\n",
    "            s3_client.upload_file(\"temp.jpg\", target_bucket, str(volume[\"identifier\"]) + '-' + padded_number + \".jpg\", ExtraArgs={'ContentType': \"image/jpeg\", 'Metadata': {\"width\": str(image[\"width\"]), \"height\": str(image[\"height\"])}})\n",
    "            images += 1\n",
    "            \n",
    "    os.remove(\"temp.jpg\")\n",
    "            \n",
    "    return str(images) + \" images copied from \" + source_bucket + \" to \" + target_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "convertible-situation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now working on 20387\n",
      "Now working on 20475\n",
      "Now working on 21251\n",
      "Now working on 21252\n",
      "Now working on 21954\n",
      "Now working on 21963\n",
      "Now working on 22029\n",
      "Now working on 22152\n",
      "Now working on 22370\n",
      "Now working on 22614\n",
      "Now working on 24233\n",
      "Now working on 24335\n",
      "Now working on 24352\n",
      "Now working on 24473\n",
      "Now working on 24474\n",
      "Now working on 24671\n",
      "Now working on 24672\n",
      "Now working on 24673\n",
      "Now working on 24674\n",
      "Now working on 25291\n",
      "Now working on 25292\n",
      "Now working on 25344\n",
      "Now working on 25470\n",
      "Now working on 26267\n",
      "Now working on 26268\n",
      "Now working on 26269\n",
      "Now working on 26718\n",
      "Now working on 27140\n",
      "Now working on 27576\n",
      "Now working on 27577\n",
      "Now working on 27578\n",
      "Now working on 27579\n",
      "Now working on 27580\n",
      "Now working on 27581\n",
      "Now working on 306662\n",
      "Now working on 324571\n",
      "Now working on 324890\n",
      "Now working on 325192\n",
      "Now working on 325601\n",
      "Now working on 326363\n",
      "Now working on 326519\n",
      "Now working on 326801\n",
      "Now working on 327049\n",
      "Now working on 327399\n",
      "Now working on 327693\n",
      "Now working on 153874\n",
      "Now working on 154082\n",
      "Now working on 154414\n",
      "Now working on 154441\n",
      "Now working on 154751\n",
      "Now working on 135009\n",
      "Now working on 305395\n",
      "Now working on 305447\n",
      "Now working on 305685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'14403 images copied from ssda-assets to ssda-production-jpgs'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "copy_jpgs(\"cuba.json\", \"ssda-assets\", \"ssda-production-jpgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unsigned-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def build_volume_records(json_path, target_bucket):\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_client = boto3.client('s3')\n",
    "    #bucket = s3_resource.Bucket(target_bucket)    \n",
    "    \n",
    "    volumes = 0\n",
    "    \n",
    "    with open(json_path, encoding=\"utf-8\") as jsonfile:\n",
    "        data = json.load(jsonfile)\n",
    "        \n",
    "    for volume in data[\"volumes\"]:\n",
    "        keys_to_delete = [\"s3_path\", \"contributor\"]\n",
    "        keys_to_add = []\n",
    "        values_to_add = []\n",
    "        for key in volume:\n",
    "            if key in keys_to_delete:                \n",
    "                continue\n",
    "            elif (type(volume[key]) == list) and ((key == \"creator\") or (key == \"format\")):                \n",
    "                temp = ''\n",
    "                for element in volume[key]:\n",
    "                    temp += element + \"; \"\n",
    "                temp = temp[:len(temp) - 2]\n",
    "                volume[key] = temp\n",
    "            elif key == \"date\":\n",
    "                if volume[key][\"start year\"] == None:\n",
    "                    start_date = \"1500\"\n",
    "                else:\n",
    "                    start_date = str(volume[key][\"start year\"])\n",
    "                if volume[key][\"start month\"] != None:\n",
    "                    str_start_month = str(volume[key][\"start month\"])\n",
    "                    start_date += '-' + '0' * (2 - len(str_start_month)) + str_start_month\n",
    "                    if volume[key][\"start day\"] != None:\n",
    "                        str_start_day = str(volume[key][\"start day\"])\n",
    "                        start_date += '-' + '0' * (2 - len(str_start_day)) + str_start_day\n",
    "                if volume[key][\"end year\"] == None:\n",
    "                    end_date = \"2000\"\n",
    "                else:\n",
    "                    end_date = str(volume[key][\"end year\"])\n",
    "                if volume[key][\"end month\"] != None:\n",
    "                    str_end_month = str(volume[key][\"end month\"])\n",
    "                    end_date += '-' + '0' * (2 - len(str_end_month)) + str_end_month\n",
    "                    if volume[key][\"end day\"] != None:\n",
    "                        str_end_day = str(volume[key][\"end day\"])\n",
    "                        end_date += '-' + '0' * (2 - len(str_end_day)) + str_end_day                \n",
    "                keys_to_delete.append(key)                \n",
    "            elif type(volume[key]) == dict:\n",
    "                for key_key in volume[key]:\n",
    "                    keys_to_add.append(key_key)\n",
    "                    values_to_add.append(volume[key][key_key])\n",
    "                keys_to_delete.append(key)\n",
    "            elif volume[key] == None:\n",
    "                volume[key] = ''\n",
    "                \n",
    "        for i in range(len(keys_to_add)):\n",
    "            if values_to_add[i] == None:\n",
    "                values_to_add[i] = ''\n",
    "            \n",
    "            if (keys_to_add[i] == \"coords\") and (values_to_add[i] != ''):\n",
    "                coords = values_to_add[i].split(',')\n",
    "                volume[keys_to_add[i]] = '(' + coords[0] + \", \" + coords[1] + ')'\n",
    "                continue\n",
    "            elif keys_to_add[i] == \"coords\":\n",
    "                volume[keys_to_add[i]] = \"(0,0)\"\n",
    "                continue\n",
    "                \n",
    "            volume[keys_to_add[i]] = values_to_add[i]\n",
    "            \n",
    "        volume[\"start_date\"] = start_date\n",
    "        volume[\"end_date\"] = end_date\n",
    "        \n",
    "        for key in keys_to_delete:            \n",
    "            del volume[key]\n",
    "                    \n",
    "        with open(\"temp.json\", 'w', encoding=\"utf-8\") as outfile:\n",
    "            json.dump(volume, outfile)               \n",
    "        s3_client.upload_file(\"temp.json\", target_bucket, str(volume[\"identifier\"]) + \".json\", ExtraArgs={'ContentType': \"application/json\"})\n",
    "        volumes += 1\n",
    "        \n",
    "    return \"Metadata for \" + str(volumes) + \" volumes uploaded to S3.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sensitive-belle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Metadata for 30 volumes uploaded to S3.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "build_volume_records(\"us-vol.json\", \"ssda-volume-metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "textile-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def build_cloudsearch_batch(json_path, output_dir):\n",
    "    \n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_client = boto3.client('s3')\n",
    "    #bucket = s3_resource.Bucket(target_bucket)    \n",
    "    \n",
    "    volumes = 0\n",
    "    batch = []\n",
    "    \n",
    "    with open(json_path, encoding=\"utf-8\") as jsonfile:\n",
    "        data = json.load(jsonfile)\n",
    "        \n",
    "    for volume in data[\"volumes\"]:\n",
    "        keys_to_delete = [\"s3_path\", \"contributor\"]\n",
    "        keys_to_add = []\n",
    "        values_to_add = []\n",
    "        for key in volume:\n",
    "            if key in keys_to_delete:                \n",
    "                continue\n",
    "            elif (type(volume[key]) == list) and ((key == \"creator\") or (key == \"format\")):                \n",
    "                temp = ''\n",
    "                for element in volume[key]:\n",
    "                    temp += element + \"; \"\n",
    "                temp = temp[:len(temp) - 2]\n",
    "                volume[key] = temp\n",
    "            elif key == \"date\":\n",
    "                if volume[key][\"start year\"] == None:\n",
    "                    start_date = \"1500\"\n",
    "                else:\n",
    "                    start_date = str(volume[key][\"start year\"])\n",
    "                if volume[key][\"start month\"] != None:\n",
    "                    str_start_month = str(volume[key][\"start month\"])\n",
    "                    start_date += '-' + '0' * (2 - len(str_start_month)) + str_start_month\n",
    "                    if volume[key][\"start day\"] != None:\n",
    "                        str_start_day = str(volume[key][\"start day\"])\n",
    "                        start_date += '-' + '0' * (2 - len(str_start_day)) + str_start_day\n",
    "                if volume[key][\"end year\"] == None:\n",
    "                    end_date = \"2000\"\n",
    "                else:\n",
    "                    end_date = str(volume[key][\"end year\"])\n",
    "                if volume[key][\"end month\"] != None:\n",
    "                    str_end_month = str(volume[key][\"end month\"])\n",
    "                    end_date += '-' + '0' * (2 - len(str_end_month)) + str_end_month\n",
    "                    if volume[key][\"end day\"] != None:\n",
    "                        str_end_day = str(volume[key][\"end day\"])\n",
    "                        end_date += '-' + '0' * (2 - len(str_end_day)) + str_end_day                \n",
    "                keys_to_delete.append(key)                \n",
    "            elif type(volume[key]) == dict:\n",
    "                for key_key in volume[key]:\n",
    "                    keys_to_add.append(key_key)\n",
    "                    values_to_add.append(volume[key][key_key])\n",
    "                keys_to_delete.append(key)\n",
    "            elif volume[key] == None:\n",
    "                volume[key] = ''\n",
    "                \n",
    "        for i in range(len(keys_to_add)):\n",
    "            if values_to_add[i] == None:\n",
    "                values_to_add[i] = ''\n",
    "            \n",
    "            if (keys_to_add[i] == \"coords\") and (values_to_add[i] != ''):\n",
    "                coords = values_to_add[i].split(',')\n",
    "                volume[keys_to_add[i]] = coords[0] + \", \" + coords[1]\n",
    "                continue\n",
    "            elif keys_to_add[i] == \"coords\":\n",
    "                volume[keys_to_add[i]] = \"0, 0\"\n",
    "                continue\n",
    "                \n",
    "            volume[keys_to_add[i]] = values_to_add[i]\n",
    "            \n",
    "        if len(start_date) == 10:\n",
    "            volume[\"start_date\"] = start_date + \"T00:00:00Z\"\n",
    "        elif len(start_date) == 7:\n",
    "            volume[\"start_date\"] = start_date + \"-01T00:00:00Z\"\n",
    "        else:\n",
    "            volume[\"start_date\"] = start_date + \"-01-01T00:00:00Z\"\n",
    "        \n",
    "        date_parts = end_date.split('-')\n",
    "        long_months = [\"01\", \"03\", \"05\", \"07\", \"08\", \"10\", \"12\"]\n",
    "        if len(date_parts) == 2:\n",
    "            if date_parts[1] in long_months:\n",
    "                end_month_length = \"31\"\n",
    "            elif date_parts[1] == \"02\":\n",
    "                end_month_length = \"28\"\n",
    "            else:\n",
    "                end_month_length = \"30\"\n",
    "        \n",
    "        if len(end_date) == 10:\n",
    "            volume[\"end_date\"] = end_date + \"T23:59:59Z\"\n",
    "        elif len(end_date) == 7:\n",
    "            volume[\"end_date\"] = end_date + end_month_length + \"T23:59:59Z\"\n",
    "        else:\n",
    "            volume[\"end_date\"] = end_date + \"-12-31T23:59:59Z\"\n",
    "        \n",
    "        for key in keys_to_delete:            \n",
    "            del volume[key]\n",
    "                    \n",
    "        batch_record = {\"type\": \"add\", \"id\": str(volume[\"identifier\"]), \"fields\": volume}\n",
    "        batch.append(batch_record)\n",
    "        volumes += 1\n",
    "        \n",
    "    with open(output_dir, 'w', encoding=\"utf-8\") as outfile:\n",
    "        json.dump(batch, outfile)   \n",
    "        \n",
    "    return \"Metadata for \" + str(volumes) + \" volumes combined in a CloudSearch batch request available at \" + output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "comparative-comparative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Metadata for 30 volumes combined in a CloudSearch batch request available at us-cs-batch.json'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "build_cloudsearch_batch(\"us-vol.json\", \"us-cs-batch.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def scrape_bucket(bucket_name, prefix=None):\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_client = boto3.client('s3')\n",
    "    bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    volume_ids = []\n",
    "    titles = []\n",
    "    volume_roots = []\n",
    "    image_counts = []\n",
    "    has_jpg = []\n",
    "    has_tif = []\n",
    "    has_other = []\n",
    "    other = []\n",
    "    has_pdf = []\n",
    "    has_metadata = []\n",
    "    volume_metadata = []\n",
    "    \n",
    "    folders = [\"jpg\", \"tif\", \"metadata\"]   \n",
    "    \n",
    "    for obj in bucket.objects.filter(Prefix = prefix):    \n",
    "        if (len(obj.key.split('/')) >= 4) and (obj.key.split('/')[3].isdigit()) and (obj.key.split('/')[3] != '') and (obj.key.split('/')[3] not in volume_ids):\n",
    "            volume_ids.append(obj.key.split('/')[3])\n",
    "            volume_root = obj.key[:obj.key.find('/', obj.key.find(obj.key.split('/')[3]))]\n",
    "            volume_roots.append(volume_root)            \n",
    "            has_metadata.append(False)\n",
    "            has_pdf.append(False)\n",
    "            has_jpg.append(False)\n",
    "            has_tif.append(False)\n",
    "            has_other.append(False)\n",
    "            other.append(None)\n",
    "            for volume_obj in bucket.objects.filter(Prefix = volume_root):\n",
    "                if \"DC.xml\" in volume_obj.key:                    \n",
    "                    has_metadata[-1] = True\n",
    "                    s3_client.download_file(bucket.name, volume_obj.key, \"temp.xml\")\n",
    "                    vol_dict = ssda_volume_xml_to_dict(\"temp.xml\", volume_root)\n",
    "                    volume_metadata.append(vol_dict)\n",
    "                    if \"title\" in vol_dict:\n",
    "                        titles.append(vol_dict[\"title\"])\n",
    "                    else:\n",
    "                        titles.append(\"no title\")\n",
    "                    os.remove(\"temp.xml\")\n",
    "                elif \"pdf\" in volume_obj.key.lower():\n",
    "                    has_pdf[-1] = True\n",
    "                elif (has_jpg[-1] == False) and (volume_obj.key.lower().split('/')[4] == \"jpg\"):\n",
    "                    has_jpg[-1] = True\n",
    "                elif (has_tif[-1] == False) and (volume_obj.key.lower().split('/')[4] == \"tif\"):\n",
    "                    has_tif[-1] = True\n",
    "                elif (len(volume_obj.key.split('/')) > 5) and (volume_obj.key.lower().split('/')[4] not in folders) and ((other[-1] == None) or (volume_obj.key.lower().split('/')[4] not in other[-1])):\n",
    "                    has_other[-1] = True\n",
    "                    if other[-1] == None:\n",
    "                        other[-1] = volume_obj.key.lower().split('/')[4]\n",
    "                    else:\n",
    "                        other[-1] = other[-1] + '|' + volume_obj.key.lower().split('/')[4]\n",
    "                \n",
    "            image_metadata = []\n",
    "            prod_imgs = None\n",
    "            if has_jpg[-1]:\n",
    "                prod_imgs = \"jpg\"\n",
    "            elif has_tif[-1]:\n",
    "                prod_imgs = \"tif\"\n",
    "                \n",
    "            if prod_imgs == None:\n",
    "                volume_metadata[-1][\"images\"] = []\n",
    "                image_counts.append(0)\n",
    "            else:\n",
    "                bad_images = 0\n",
    "                for image_obj in bucket.objects.filter(Prefix = volume_root + '/' + prod_imgs.upper()):\n",
    "                    if ('.' + prod_imgs) in image_obj.key.lower():\n",
    "                        file_name = image_obj.key[image_obj.key.rfind('/') + 1:image_obj.key.rfind('.')]\n",
    "                        if not file_name.isdigit():\n",
    "                            print(\"found bad image file name at \" + image_obj.key)\n",
    "                            bad_images += 1\n",
    "                            continue\n",
    "                        extension = image_obj.key[image_obj.key.rfind('.') + 1:]\n",
    "                        temp_path = file_name + '.' + extension\n",
    "                        s3_client.download_file(bucket.name, image_obj.key, temp_path)\n",
    "                        im = Image.open(temp_path)\n",
    "                        width, height = im.size\n",
    "                        im.close()\n",
    "                        os.remove(temp_path)\n",
    "                        image = {\"file_name\": int(file_name), \"extension\": extension, \"height\": height, \"width\": width}\n",
    "                        image_metadata.append(image)\n",
    "                volume_metadata[-1][\"images\"] = image_metadata\n",
    "                image_counts.append(len(image_metadata) + bad_images)           \n",
    "            \n",
    "            print(\"Completed \" + titles[-1])\n",
    "        elif (len(obj.key.split('/')) >= 5) and (not obj.key.split('/')[3].isdigit()) and (obj.key.split('/')[4] != '') and (obj.key.split('/')[4] not in volume_ids):\n",
    "            volume_ids.append(obj.key.split('/')[4])\n",
    "            volume_root = obj.key[:obj.key.find('/', obj.key.find(obj.key.split('/')[4]))]\n",
    "            volume_roots.append(volume_root)            \n",
    "            has_metadata.append(False)\n",
    "            has_pdf.append(False)\n",
    "            has_jpg.append(False)\n",
    "            has_tif.append(False)\n",
    "            has_other.append(False)\n",
    "            other.append(None)\n",
    "            for volume_obj in bucket.objects.filter(Prefix = volume_root):\n",
    "                if \"DC.xml\" in volume_obj.key:                    \n",
    "                    has_metadata[-1] = True\n",
    "                    s3_client.download_file(bucket.name, volume_obj.key, \"temp.xml\")\n",
    "                    vol_dict = ssda_volume_xml_to_dict(\"temp.xml\", volume_root)\n",
    "                    volume_metadata.append(vol_dict)\n",
    "                    if (\"title\" in vol_dict) and (vol_dict[\"title\"] != None):\n",
    "                        titles.append(vol_dict[\"title\"])\n",
    "                    else:\n",
    "                        titles.append(\"no title\")\n",
    "                    os.remove(\"temp.xml\")\n",
    "                elif \"pdf\" in volume_obj.key.lower():\n",
    "                    has_pdf[-1] = True\n",
    "                elif (has_jpg[-1] == False) and (volume_obj.key.lower().split('/')[5] == \"jpg\"):\n",
    "                    has_jpg[-1] = True\n",
    "                elif (has_tif[-1] == False) and (volume_obj.key.lower().split('/')[5] == \"tif\"):\n",
    "                    has_tif[-1] = True\n",
    "                elif (len(volume_obj.key.split('/')) > 6) and (volume_obj.key.lower().split('/')[5] not in folders) and ((other[-1] == None) or (volume_obj.key.lower().split('/')[5] not in other[-1])):\n",
    "                    has_other[-1] = True\n",
    "                    if other[-1] == None:\n",
    "                        other[-1] = volume_obj.key.lower().split('/')[5]\n",
    "                    else:\n",
    "                        other[-1] = other[-1] + '|' + volume_obj.key.lower().split('/')[5]\n",
    "                        \n",
    "            if has_metadata[-1] == False:\n",
    "                titles.append(\"no title\")\n",
    "                print(\"Failed to find metadata for \" + volume_root)\n",
    "                \n",
    "            image_metadata = []\n",
    "            prod_imgs = None\n",
    "            if has_jpg[-1]:\n",
    "                prod_imgs = \"jpg\"\n",
    "            elif has_tif[-1]:\n",
    "                prod_imgs = \"tif\"\n",
    "                \n",
    "            if prod_imgs == None:\n",
    "                volume_metadata[-1][\"images\"] = []\n",
    "                image_counts.append(0)\n",
    "            else:\n",
    "                bad_images = 0\n",
    "                for image_obj in bucket.objects.filter(Prefix = volume_root + '/' + prod_imgs.upper()):\n",
    "                    if ('.' + prod_imgs) in image_obj.key.lower():\n",
    "                        file_name = image_obj.key[image_obj.key.rfind('/') + 1:image_obj.key.rfind('.')]\n",
    "                        if not file_name.isdigit():\n",
    "                            print(\"found incorrect image file name at \" + image_obj.key)\n",
    "                            bad_images += 1\n",
    "                            continue\n",
    "                        extension = image_obj.key[image_obj.key.rfind('.') + 1:]\n",
    "                        temp_path = file_name + '.' + extension\n",
    "                        s3_client.download_file(bucket.name, image_obj.key, temp_path)\n",
    "                        try:\n",
    "                            im = Image.open(temp_path)\n",
    "                            width, height = im.size\n",
    "                            im.close()                        \n",
    "                            image = {\"file_name\": int(file_name), \"extension\": extension, \"height\": height, \"width\": width}\n",
    "                            image_metadata.append(image)\n",
    "                        except:\n",
    "                            print(\"found bad image file at \" + image_obj.key)\n",
    "                            bad_images += 1\n",
    "                        os.remove(temp_path)\n",
    "                volume_metadata[-1][\"images\"] = image_metadata\n",
    "                image_counts.append(len(image_metadata) + bad_images)           \n",
    "            \n",
    "            try:\n",
    "                print(\"Completed \" + titles[-1])\n",
    "            except:\n",
    "                print(\"Completed\")\n",
    "                print(titles[-1])\n",
    "                \n",
    "    volumes_dict = {\"id\": volume_ids, \"title\": titles, \"images\": image_counts, \"s3 root\": volume_roots, \"metadata\": has_metadata, \"has pdf\": has_pdf, \"has jpg\": has_jpg, \"has tif\": has_tif, \"has other\": has_other, \"other\": other}\n",
    "    volumes_df = pd.DataFrame.from_dict(volumes_dict)\n",
    "    \n",
    "    return volumes_df, volume_metadata  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def ssda_volume_xml_to_dict(volume_xml, s3_path):\n",
    "    import xml.etree.ElementTree as ET\n",
    "    tree = ET.parse(volume_xml)\n",
    "    root = tree.getroot()\n",
    "    volume_dict = {}\n",
    "    volume_dict[\"s3_path\"] = s3_path\n",
    "    for item in root:\n",
    "        if \"{http://purl.org/dc/elements/1.1/}\" in item.tag:\n",
    "            item.tag = item.tag[item.tag.find('}') + 1:]\n",
    "        if item.text == None:\n",
    "            if item.tag not in volume_dict:\n",
    "                volume_dict[item.tag] = None\n",
    "            continue\n",
    "        if item.text[0] == ' ':\n",
    "            item.text = item.text[1:]        \n",
    "        if item.tag == \"subject\":\n",
    "            if \"subject\" in volume_dict:\n",
    "                volume_dict[\"subject\"].append(item.text.split(\"--\"))\n",
    "            else:\n",
    "                volume_dict[\"subject\"] = item.text.split(\"--\")\n",
    "        elif item.tag == \"title\":\n",
    "            volume_dict[\"title\"] = item.text\n",
    "        elif item.tag == \"contributor\":\n",
    "            if (item.text.find('(') != -1) and (item.text.find(')') != -1):\n",
    "                name = item.text[:item.text.find('(')]\n",
    "                role = item.text[item.text.find('(') + 1:item.text.find(')')]\n",
    "            else:\n",
    "                continue\n",
    "            if \"contributor\" in volume_dict:\n",
    "                volume_dict[\"contributor\"].append({\"name\": name, \"role\": role})\n",
    "            else:\n",
    "                volume_dict[\"contributor\"] = [{\"name\": name, \"role\": role}]\n",
    "        elif item.tag == \"identifier\":\n",
    "            volume_dict[\"identifier\"] = item.text[item.text.find(':') + 1:]\n",
    "        elif item.tag == \"coverage\":\n",
    "            if ('.' in item.text) and (',' in item.text) and (\"Archives\" not in item.text):\n",
    "                if \"coverage\" in volume_dict:\n",
    "                    volume_dict[\"coverage\"][\"coords\"] = item.text\n",
    "                else:\n",
    "                    volume_dict[\"coverage\"] = {\"coords\": item.text}\n",
    "            elif \"--\" in item.text:\n",
    "                places = item.text.split(\"--\")\n",
    "                if len(places) == 4:\n",
    "                    if \"coverage\" in volume_dict:\n",
    "                        volume_dict[\"coverage\"][\"country\"] = places[1]\n",
    "                        volume_dict[\"coverage\"][\"state\"] = places[2]\n",
    "                        volume_dict[\"coverage\"][\"city\"] = places[3]\n",
    "                    else:\n",
    "                        volume_dict[\"coverage\"] = {\"country\": places[1], \"state\": places[2], \"city\": places[3]}                \n",
    "        elif item.tag == \"source\":\n",
    "            if \"coverage\" in volume_dict:\n",
    "                volume_dict[\"coverage\"][\"institution\"] = item.text\n",
    "            else:\n",
    "                volume_dict[\"coverage\"] = {\"institution\": item.text}\n",
    "        elif ((item.tag == \"type\") and (item.text == \"Text\")) or (item.tag == \"rights\"):\n",
    "            continue\n",
    "        elif (item.tag == \"creator\") and (';' in item.text):            \n",
    "            creators = item.text.split(';')\n",
    "            for creator in creators:                \n",
    "                if (len(creator) > 1) and (creator[0] == ' '):\n",
    "                    creator = creator[1:]\n",
    "                if \"creator\" in volume_dict:\n",
    "                    volume_dict[\"creator\"].append(creator)\n",
    "                else:\n",
    "                    volume_dict[\"creator\"] = [creator]\n",
    "        elif (item.tag == \"language\") and (';' in item.text):\n",
    "            languages = item.text.split(';')\n",
    "            for language in languages:\n",
    "                if (len(language) > 1) and (language[0] == ' '):\n",
    "                    language = language[1:]\n",
    "                if \"language\" in volume_dict:\n",
    "                    volume_dict[\"language\"].append(language)\n",
    "                else:\n",
    "                    volume_dict[\"language\"] = [language]\n",
    "        else:            \n",
    "            if item.tag in volume_dict:                \n",
    "                volume_dict[item.tag].append(item.text)                \n",
    "            else:\n",
    "                volume_dict[item.tag] = [item.text]       \n",
    "        \n",
    "    if \"coverage\" not in volume_dict:\n",
    "        volume_dict[\"coverage\"] = {}               \n",
    "        volume_dict[\"coverage\"][\"country\"] = volume_dict[\"s3_path\"].split('/')[0].replace('_', ' ')\n",
    "        volume_dict[\"coverage\"][\"state\"] = volume_dict[\"s3_path\"].split('/')[1].replace('_', ' ')\n",
    "        volume_dict[\"coverage\"][\"city\"] = volume_dict[\"s3_path\"].split('/')[2].replace('_', ' ')\n",
    "        volume_dict[\"coverage\"][\"institution\"] = volume_dict[\"s3_path\"].split('/')[3].replace('_', ' ')\n",
    "    elif \"institution\" not in volume_dict[\"coverage\"]:\n",
    "        volume_dict[\"coverage\"][\"institution\"] = volume_dict[\"s3_path\"].split('/')[3].replace('_', ' ')\n",
    "        \n",
    "    return volume_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-peripheral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Libro de Venta de Esclavo\n",
      "Finished working on Colombia_Chocó_Quibdó_Notaria_Primera_de_Quibdó_5697\n"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "prefixes = [\"Colombia/Chocó/Quibdó/Notaria_Primera_de_Quibdó/56975\"]\n",
    "\n",
    "for pref in prefixes:\n",
    "    df, metadata = scrape_bucket(\"ssda-assets\", prefix=pref)\n",
    "    pref = pref.replace('/', '_')\n",
    "    pref = pref[:-1]\n",
    "    df.to_csv(pref.lower() + \".csv\", index = False)\n",
    "    with open(pref.lower() + \".json\", 'w', encoding=\"utf-8\") as outfile:\n",
    "        outfile.write('{\\n\\\"volumes\\\": \\n')\n",
    "        json.dump(metadata, outfile)\n",
    "        outfile.write('}')\n",
    "    print(\"Finished working on \" + pref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-concert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted manifest_generation.ipynb.\n",
      "Converted s3_scrape.ipynb.\n",
      "Converted s3_scrape_dev.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-consensus",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
